{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7214</th>\n",
       "      <td>0.051633</td>\n",
       "      <td>-0.008142</td>\n",
       "      <td>-0.024332</td>\n",
       "      <td>0.013864</td>\n",
       "      <td>0.044475</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>-0.052417</td>\n",
       "      <td>0.025871</td>\n",
       "      <td>0.068986</td>\n",
       "      <td>-0.006843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021133</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>-0.009949</td>\n",
       "      <td>-0.055740</td>\n",
       "      <td>-0.029697</td>\n",
       "      <td>-0.020551</td>\n",
       "      <td>0.041049</td>\n",
       "      <td>0.016886</td>\n",
       "      <td>-0.029579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>0.034698</td>\n",
       "      <td>-0.087794</td>\n",
       "      <td>-0.008092</td>\n",
       "      <td>-0.039379</td>\n",
       "      <td>0.028995</td>\n",
       "      <td>-0.021274</td>\n",
       "      <td>0.029689</td>\n",
       "      <td>-0.011294</td>\n",
       "      <td>0.052413</td>\n",
       "      <td>-0.041411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030786</td>\n",
       "      <td>-0.015544</td>\n",
       "      <td>0.008737</td>\n",
       "      <td>-0.049981</td>\n",
       "      <td>-0.103033</td>\n",
       "      <td>0.019641</td>\n",
       "      <td>0.009213</td>\n",
       "      <td>0.016622</td>\n",
       "      <td>-0.023977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>0.064077</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>-0.008914</td>\n",
       "      <td>0.016467</td>\n",
       "      <td>0.039864</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>-0.062220</td>\n",
       "      <td>0.032777</td>\n",
       "      <td>0.083319</td>\n",
       "      <td>-0.017697</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026584</td>\n",
       "      <td>0.026626</td>\n",
       "      <td>0.012260</td>\n",
       "      <td>-0.066954</td>\n",
       "      <td>-0.048784</td>\n",
       "      <td>-0.034600</td>\n",
       "      <td>0.021107</td>\n",
       "      <td>0.030552</td>\n",
       "      <td>-0.021320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6812</th>\n",
       "      <td>0.048182</td>\n",
       "      <td>-0.069188</td>\n",
       "      <td>-0.023972</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.016139</td>\n",
       "      <td>-0.006851</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.069693</td>\n",
       "      <td>0.034364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>-0.040926</td>\n",
       "      <td>-0.005258</td>\n",
       "      <td>-0.028718</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>0.019760</td>\n",
       "      <td>0.024238</td>\n",
       "      <td>-0.020872</td>\n",
       "      <td>-0.040295</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>0.066079</td>\n",
       "      <td>-0.050984</td>\n",
       "      <td>-0.029105</td>\n",
       "      <td>0.020741</td>\n",
       "      <td>0.018208</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>-0.063394</td>\n",
       "      <td>0.043426</td>\n",
       "      <td>0.030005</td>\n",
       "      <td>-0.012341</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032436</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>0.029041</td>\n",
       "      <td>-0.003624</td>\n",
       "      <td>-0.006724</td>\n",
       "      <td>0.030850</td>\n",
       "      <td>0.007316</td>\n",
       "      <td>-0.011735</td>\n",
       "      <td>-0.015874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>0.071723</td>\n",
       "      <td>-0.022951</td>\n",
       "      <td>-0.009669</td>\n",
       "      <td>-0.008356</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>-0.038034</td>\n",
       "      <td>0.035612</td>\n",
       "      <td>0.069025</td>\n",
       "      <td>-0.024401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013278</td>\n",
       "      <td>-0.001090</td>\n",
       "      <td>-0.058875</td>\n",
       "      <td>-0.088182</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.027868</td>\n",
       "      <td>0.030174</td>\n",
       "      <td>-0.016046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>0.087692</td>\n",
       "      <td>-0.021290</td>\n",
       "      <td>-0.025321</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>-0.008885</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>-0.019462</td>\n",
       "      <td>-0.002037</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>-0.030091</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010586</td>\n",
       "      <td>0.033286</td>\n",
       "      <td>0.023939</td>\n",
       "      <td>-0.062817</td>\n",
       "      <td>-0.037413</td>\n",
       "      <td>-0.013525</td>\n",
       "      <td>0.032097</td>\n",
       "      <td>-0.005044</td>\n",
       "      <td>-0.027736</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>0.078633</td>\n",
       "      <td>-0.008377</td>\n",
       "      <td>-0.026783</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.018558</td>\n",
       "      <td>-0.002488</td>\n",
       "      <td>0.052191</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004679</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>-0.040454</td>\n",
       "      <td>-0.079680</td>\n",
       "      <td>0.025330</td>\n",
       "      <td>0.023660</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>-0.028357</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>0.057288</td>\n",
       "      <td>-0.011589</td>\n",
       "      <td>-0.008272</td>\n",
       "      <td>-0.017983</td>\n",
       "      <td>0.037853</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>-0.054403</td>\n",
       "      <td>0.052510</td>\n",
       "      <td>0.063096</td>\n",
       "      <td>-0.023402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046968</td>\n",
       "      <td>-0.027560</td>\n",
       "      <td>0.014922</td>\n",
       "      <td>-0.033768</td>\n",
       "      <td>0.032345</td>\n",
       "      <td>-0.009779</td>\n",
       "      <td>0.021077</td>\n",
       "      <td>0.022465</td>\n",
       "      <td>-0.021588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7505</th>\n",
       "      <td>0.093012</td>\n",
       "      <td>-0.015717</td>\n",
       "      <td>-0.027347</td>\n",
       "      <td>0.021229</td>\n",
       "      <td>0.017567</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>-0.029287</td>\n",
       "      <td>0.026982</td>\n",
       "      <td>0.045942</td>\n",
       "      <td>-0.002156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>-0.012877</td>\n",
       "      <td>0.010124</td>\n",
       "      <td>-0.034091</td>\n",
       "      <td>-0.078648</td>\n",
       "      <td>-0.001790</td>\n",
       "      <td>0.029020</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>-0.041205</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9324 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "7214  0.051633 -0.008142 -0.024332  0.013864  0.044475  0.006016 -0.052417   \n",
       "5884  0.034698 -0.087794 -0.008092 -0.039379  0.028995 -0.021274  0.029689   \n",
       "5651  0.064077  0.007670 -0.008914  0.016467  0.039864  0.021567 -0.062220   \n",
       "6812  0.048182 -0.069188 -0.023972  0.010279  0.016139 -0.006851  0.008167   \n",
       "3669  0.066079 -0.050984 -0.029105  0.020741  0.018208  0.018970 -0.063394   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1644  0.071723 -0.022951 -0.009669 -0.008356  0.000957  0.000765 -0.038034   \n",
       "677   0.087692 -0.021290 -0.025321  0.048477 -0.008885  0.007916 -0.019462   \n",
       "7800  0.078633 -0.008377 -0.026783  0.014870  0.009892  0.000110 -0.018558   \n",
       "6499  0.057288 -0.011589 -0.008272 -0.017983  0.037853  0.019753 -0.054403   \n",
       "7505  0.093012 -0.015717 -0.027347  0.021229  0.017567  0.003263 -0.029287   \n",
       "\n",
       "             7         8         9  ...       759       760       761  \\\n",
       "7214  0.025871  0.068986 -0.006843  ... -0.021133  0.009711 -0.009949   \n",
       "5884 -0.011294  0.052413 -0.041411  ... -0.030786 -0.015544  0.008737   \n",
       "5651  0.032777  0.083319 -0.017697  ... -0.026584  0.026626  0.012260   \n",
       "6812  0.008073  0.069693  0.034364  ... -0.047714 -0.040926 -0.005258   \n",
       "3669  0.043426  0.030005 -0.012341  ... -0.032436  0.021301  0.029041   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1644  0.035612  0.069025 -0.024401  ... -0.023546 -0.013278 -0.001090   \n",
       "677  -0.002037  0.098750 -0.030091  ... -0.010586  0.033286  0.023939   \n",
       "7800 -0.002488  0.052191  0.012958  ... -0.004679  0.010054  0.007563   \n",
       "6499  0.052510  0.063096 -0.023402  ... -0.046968 -0.027560  0.014922   \n",
       "7505  0.026982  0.045942 -0.002156  ...  0.004115 -0.012877  0.010124   \n",
       "\n",
       "           762       763       764       765       766       767  predict  \n",
       "7214 -0.055740 -0.029697 -0.020551  0.041049  0.016886 -0.029579        0  \n",
       "5884 -0.049981 -0.103033  0.019641  0.009213  0.016622 -0.023977        0  \n",
       "5651 -0.066954 -0.048784 -0.034600  0.021107  0.030552 -0.021320        0  \n",
       "6812 -0.028718  0.014025  0.019760  0.024238 -0.020872 -0.040295        1  \n",
       "3669 -0.003624 -0.006724  0.030850  0.007316 -0.011735 -0.015874        1  \n",
       "...        ...       ...       ...       ...       ...       ...      ...  \n",
       "1644 -0.058875 -0.088182  0.001628  0.027868  0.030174 -0.016046        0  \n",
       "677  -0.062817 -0.037413 -0.013525  0.032097 -0.005044 -0.027736        2  \n",
       "7800 -0.040454 -0.079680  0.025330  0.023660  0.022532 -0.028357        2  \n",
       "6499 -0.033768  0.032345 -0.009779  0.021077  0.022465 -0.021588        0  \n",
       "7505 -0.034091 -0.078648 -0.001790  0.029020 -0.000820 -0.041205        1  \n",
       "\n",
       "[9324 rows x 769 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_row=pd.read_csv(\"/Users/withmocha/Desktop/DATA/Capston Design(2024)/data(sentence)/0426/train/sentence 5/train_data_after_vector.csv\",index_col=0)\n",
    "\n",
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=pd.DataFrame(data=(data_row['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7214</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6812</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7505</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9324 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      predict\n",
       "7214        0\n",
       "5884        0\n",
       "5651        0\n",
       "6812        1\n",
       "3669        1\n",
       "...       ...\n",
       "1644        0\n",
       "677         2\n",
       "7800        2\n",
       "6499        0\n",
       "7505        1\n",
       "\n",
       "[9324 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row.drop(columns=['predict'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7214</th>\n",
       "      <td>0.051633</td>\n",
       "      <td>-0.008142</td>\n",
       "      <td>-0.024332</td>\n",
       "      <td>0.013864</td>\n",
       "      <td>0.044475</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>-0.052417</td>\n",
       "      <td>0.025871</td>\n",
       "      <td>0.068986</td>\n",
       "      <td>-0.006843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015038</td>\n",
       "      <td>-0.021133</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>-0.009949</td>\n",
       "      <td>-0.055740</td>\n",
       "      <td>-0.029697</td>\n",
       "      <td>-0.020551</td>\n",
       "      <td>0.041049</td>\n",
       "      <td>0.016886</td>\n",
       "      <td>-0.029579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>0.034698</td>\n",
       "      <td>-0.087794</td>\n",
       "      <td>-0.008092</td>\n",
       "      <td>-0.039379</td>\n",
       "      <td>0.028995</td>\n",
       "      <td>-0.021274</td>\n",
       "      <td>0.029689</td>\n",
       "      <td>-0.011294</td>\n",
       "      <td>0.052413</td>\n",
       "      <td>-0.041411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034887</td>\n",
       "      <td>-0.030786</td>\n",
       "      <td>-0.015544</td>\n",
       "      <td>0.008737</td>\n",
       "      <td>-0.049981</td>\n",
       "      <td>-0.103033</td>\n",
       "      <td>0.019641</td>\n",
       "      <td>0.009213</td>\n",
       "      <td>0.016622</td>\n",
       "      <td>-0.023977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>0.064077</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>-0.008914</td>\n",
       "      <td>0.016467</td>\n",
       "      <td>0.039864</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>-0.062220</td>\n",
       "      <td>0.032777</td>\n",
       "      <td>0.083319</td>\n",
       "      <td>-0.017697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007360</td>\n",
       "      <td>-0.026584</td>\n",
       "      <td>0.026626</td>\n",
       "      <td>0.012260</td>\n",
       "      <td>-0.066954</td>\n",
       "      <td>-0.048784</td>\n",
       "      <td>-0.034600</td>\n",
       "      <td>0.021107</td>\n",
       "      <td>0.030552</td>\n",
       "      <td>-0.021320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6812</th>\n",
       "      <td>0.048182</td>\n",
       "      <td>-0.069188</td>\n",
       "      <td>-0.023972</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.016139</td>\n",
       "      <td>-0.006851</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.069693</td>\n",
       "      <td>0.034364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030912</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>-0.040926</td>\n",
       "      <td>-0.005258</td>\n",
       "      <td>-0.028718</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>0.019760</td>\n",
       "      <td>0.024238</td>\n",
       "      <td>-0.020872</td>\n",
       "      <td>-0.040295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>0.066079</td>\n",
       "      <td>-0.050984</td>\n",
       "      <td>-0.029105</td>\n",
       "      <td>0.020741</td>\n",
       "      <td>0.018208</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>-0.063394</td>\n",
       "      <td>0.043426</td>\n",
       "      <td>0.030005</td>\n",
       "      <td>-0.012341</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005794</td>\n",
       "      <td>-0.032436</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>0.029041</td>\n",
       "      <td>-0.003624</td>\n",
       "      <td>-0.006724</td>\n",
       "      <td>0.030850</td>\n",
       "      <td>0.007316</td>\n",
       "      <td>-0.011735</td>\n",
       "      <td>-0.015874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>0.071723</td>\n",
       "      <td>-0.022951</td>\n",
       "      <td>-0.009669</td>\n",
       "      <td>-0.008356</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>-0.038034</td>\n",
       "      <td>0.035612</td>\n",
       "      <td>0.069025</td>\n",
       "      <td>-0.024401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001634</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013278</td>\n",
       "      <td>-0.001090</td>\n",
       "      <td>-0.058875</td>\n",
       "      <td>-0.088182</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.027868</td>\n",
       "      <td>0.030174</td>\n",
       "      <td>-0.016046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>0.087692</td>\n",
       "      <td>-0.021290</td>\n",
       "      <td>-0.025321</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>-0.008885</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>-0.019462</td>\n",
       "      <td>-0.002037</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>-0.030091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031360</td>\n",
       "      <td>-0.010586</td>\n",
       "      <td>0.033286</td>\n",
       "      <td>0.023939</td>\n",
       "      <td>-0.062817</td>\n",
       "      <td>-0.037413</td>\n",
       "      <td>-0.013525</td>\n",
       "      <td>0.032097</td>\n",
       "      <td>-0.005044</td>\n",
       "      <td>-0.027736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>0.078633</td>\n",
       "      <td>-0.008377</td>\n",
       "      <td>-0.026783</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.018558</td>\n",
       "      <td>-0.002488</td>\n",
       "      <td>0.052191</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014843</td>\n",
       "      <td>-0.004679</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>-0.040454</td>\n",
       "      <td>-0.079680</td>\n",
       "      <td>0.025330</td>\n",
       "      <td>0.023660</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>-0.028357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>0.057288</td>\n",
       "      <td>-0.011589</td>\n",
       "      <td>-0.008272</td>\n",
       "      <td>-0.017983</td>\n",
       "      <td>0.037853</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>-0.054403</td>\n",
       "      <td>0.052510</td>\n",
       "      <td>0.063096</td>\n",
       "      <td>-0.023402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006385</td>\n",
       "      <td>-0.046968</td>\n",
       "      <td>-0.027560</td>\n",
       "      <td>0.014922</td>\n",
       "      <td>-0.033768</td>\n",
       "      <td>0.032345</td>\n",
       "      <td>-0.009779</td>\n",
       "      <td>0.021077</td>\n",
       "      <td>0.022465</td>\n",
       "      <td>-0.021588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7505</th>\n",
       "      <td>0.093012</td>\n",
       "      <td>-0.015717</td>\n",
       "      <td>-0.027347</td>\n",
       "      <td>0.021229</td>\n",
       "      <td>0.017567</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>-0.029287</td>\n",
       "      <td>0.026982</td>\n",
       "      <td>0.045942</td>\n",
       "      <td>-0.002156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030717</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>-0.012877</td>\n",
       "      <td>0.010124</td>\n",
       "      <td>-0.034091</td>\n",
       "      <td>-0.078648</td>\n",
       "      <td>-0.001790</td>\n",
       "      <td>0.029020</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>-0.041205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9324 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "7214  0.051633 -0.008142 -0.024332  0.013864  0.044475  0.006016 -0.052417   \n",
       "5884  0.034698 -0.087794 -0.008092 -0.039379  0.028995 -0.021274  0.029689   \n",
       "5651  0.064077  0.007670 -0.008914  0.016467  0.039864  0.021567 -0.062220   \n",
       "6812  0.048182 -0.069188 -0.023972  0.010279  0.016139 -0.006851  0.008167   \n",
       "3669  0.066079 -0.050984 -0.029105  0.020741  0.018208  0.018970 -0.063394   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1644  0.071723 -0.022951 -0.009669 -0.008356  0.000957  0.000765 -0.038034   \n",
       "677   0.087692 -0.021290 -0.025321  0.048477 -0.008885  0.007916 -0.019462   \n",
       "7800  0.078633 -0.008377 -0.026783  0.014870  0.009892  0.000110 -0.018558   \n",
       "6499  0.057288 -0.011589 -0.008272 -0.017983  0.037853  0.019753 -0.054403   \n",
       "7505  0.093012 -0.015717 -0.027347  0.021229  0.017567  0.003263 -0.029287   \n",
       "\n",
       "             7         8         9  ...       758       759       760  \\\n",
       "7214  0.025871  0.068986 -0.006843  ... -0.015038 -0.021133  0.009711   \n",
       "5884 -0.011294  0.052413 -0.041411  ... -0.034887 -0.030786 -0.015544   \n",
       "5651  0.032777  0.083319 -0.017697  ...  0.007360 -0.026584  0.026626   \n",
       "6812  0.008073  0.069693  0.034364  ... -0.030912 -0.047714 -0.040926   \n",
       "3669  0.043426  0.030005 -0.012341  ... -0.005794 -0.032436  0.021301   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1644  0.035612  0.069025 -0.024401  ... -0.001634 -0.023546 -0.013278   \n",
       "677  -0.002037  0.098750 -0.030091  ...  0.031360 -0.010586  0.033286   \n",
       "7800 -0.002488  0.052191  0.012958  ... -0.014843 -0.004679  0.010054   \n",
       "6499  0.052510  0.063096 -0.023402  ... -0.006385 -0.046968 -0.027560   \n",
       "7505  0.026982  0.045942 -0.002156  ... -0.030717  0.004115 -0.012877   \n",
       "\n",
       "           761       762       763       764       765       766       767  \n",
       "7214 -0.009949 -0.055740 -0.029697 -0.020551  0.041049  0.016886 -0.029579  \n",
       "5884  0.008737 -0.049981 -0.103033  0.019641  0.009213  0.016622 -0.023977  \n",
       "5651  0.012260 -0.066954 -0.048784 -0.034600  0.021107  0.030552 -0.021320  \n",
       "6812 -0.005258 -0.028718  0.014025  0.019760  0.024238 -0.020872 -0.040295  \n",
       "3669  0.029041 -0.003624 -0.006724  0.030850  0.007316 -0.011735 -0.015874  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1644 -0.001090 -0.058875 -0.088182  0.001628  0.027868  0.030174 -0.016046  \n",
       "677   0.023939 -0.062817 -0.037413 -0.013525  0.032097 -0.005044 -0.027736  \n",
       "7800  0.007563 -0.040454 -0.079680  0.025330  0.023660  0.022532 -0.028357  \n",
       "6499  0.014922 -0.033768  0.032345 -0.009779  0.021077  0.022465 -0.021588  \n",
       "7505  0.010124 -0.034091 -0.078648 -0.001790  0.029020 -0.000820 -0.041205  \n",
       "\n",
       "[9324 rows x 768 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=pd.DataFrame(data_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7214</th>\n",
       "      <td>0.051633</td>\n",
       "      <td>-0.008142</td>\n",
       "      <td>-0.024332</td>\n",
       "      <td>0.013864</td>\n",
       "      <td>0.044475</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>-0.052417</td>\n",
       "      <td>0.025871</td>\n",
       "      <td>0.068986</td>\n",
       "      <td>-0.006843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015038</td>\n",
       "      <td>-0.021133</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>-0.009949</td>\n",
       "      <td>-0.055740</td>\n",
       "      <td>-0.029697</td>\n",
       "      <td>-0.020551</td>\n",
       "      <td>0.041049</td>\n",
       "      <td>0.016886</td>\n",
       "      <td>-0.029579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>0.034698</td>\n",
       "      <td>-0.087794</td>\n",
       "      <td>-0.008092</td>\n",
       "      <td>-0.039379</td>\n",
       "      <td>0.028995</td>\n",
       "      <td>-0.021274</td>\n",
       "      <td>0.029689</td>\n",
       "      <td>-0.011294</td>\n",
       "      <td>0.052413</td>\n",
       "      <td>-0.041411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034887</td>\n",
       "      <td>-0.030786</td>\n",
       "      <td>-0.015544</td>\n",
       "      <td>0.008737</td>\n",
       "      <td>-0.049981</td>\n",
       "      <td>-0.103033</td>\n",
       "      <td>0.019641</td>\n",
       "      <td>0.009213</td>\n",
       "      <td>0.016622</td>\n",
       "      <td>-0.023977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>0.064077</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>-0.008914</td>\n",
       "      <td>0.016467</td>\n",
       "      <td>0.039864</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>-0.062220</td>\n",
       "      <td>0.032777</td>\n",
       "      <td>0.083319</td>\n",
       "      <td>-0.017697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007360</td>\n",
       "      <td>-0.026584</td>\n",
       "      <td>0.026626</td>\n",
       "      <td>0.012260</td>\n",
       "      <td>-0.066954</td>\n",
       "      <td>-0.048784</td>\n",
       "      <td>-0.034600</td>\n",
       "      <td>0.021107</td>\n",
       "      <td>0.030552</td>\n",
       "      <td>-0.021320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6812</th>\n",
       "      <td>0.048182</td>\n",
       "      <td>-0.069188</td>\n",
       "      <td>-0.023972</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.016139</td>\n",
       "      <td>-0.006851</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.069693</td>\n",
       "      <td>0.034364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030912</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>-0.040926</td>\n",
       "      <td>-0.005258</td>\n",
       "      <td>-0.028718</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>0.019760</td>\n",
       "      <td>0.024238</td>\n",
       "      <td>-0.020872</td>\n",
       "      <td>-0.040295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>0.066079</td>\n",
       "      <td>-0.050984</td>\n",
       "      <td>-0.029105</td>\n",
       "      <td>0.020741</td>\n",
       "      <td>0.018208</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>-0.063394</td>\n",
       "      <td>0.043426</td>\n",
       "      <td>0.030005</td>\n",
       "      <td>-0.012341</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005794</td>\n",
       "      <td>-0.032436</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>0.029041</td>\n",
       "      <td>-0.003624</td>\n",
       "      <td>-0.006724</td>\n",
       "      <td>0.030850</td>\n",
       "      <td>0.007316</td>\n",
       "      <td>-0.011735</td>\n",
       "      <td>-0.015874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>0.071723</td>\n",
       "      <td>-0.022951</td>\n",
       "      <td>-0.009669</td>\n",
       "      <td>-0.008356</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>-0.038034</td>\n",
       "      <td>0.035612</td>\n",
       "      <td>0.069025</td>\n",
       "      <td>-0.024401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001634</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013278</td>\n",
       "      <td>-0.001090</td>\n",
       "      <td>-0.058875</td>\n",
       "      <td>-0.088182</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.027868</td>\n",
       "      <td>0.030174</td>\n",
       "      <td>-0.016046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>0.087692</td>\n",
       "      <td>-0.021290</td>\n",
       "      <td>-0.025321</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>-0.008885</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>-0.019462</td>\n",
       "      <td>-0.002037</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>-0.030091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031360</td>\n",
       "      <td>-0.010586</td>\n",
       "      <td>0.033286</td>\n",
       "      <td>0.023939</td>\n",
       "      <td>-0.062817</td>\n",
       "      <td>-0.037413</td>\n",
       "      <td>-0.013525</td>\n",
       "      <td>0.032097</td>\n",
       "      <td>-0.005044</td>\n",
       "      <td>-0.027736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>0.078633</td>\n",
       "      <td>-0.008377</td>\n",
       "      <td>-0.026783</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.018558</td>\n",
       "      <td>-0.002488</td>\n",
       "      <td>0.052191</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014843</td>\n",
       "      <td>-0.004679</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>-0.040454</td>\n",
       "      <td>-0.079680</td>\n",
       "      <td>0.025330</td>\n",
       "      <td>0.023660</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>-0.028357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>0.057288</td>\n",
       "      <td>-0.011589</td>\n",
       "      <td>-0.008272</td>\n",
       "      <td>-0.017983</td>\n",
       "      <td>0.037853</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>-0.054403</td>\n",
       "      <td>0.052510</td>\n",
       "      <td>0.063096</td>\n",
       "      <td>-0.023402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006385</td>\n",
       "      <td>-0.046968</td>\n",
       "      <td>-0.027560</td>\n",
       "      <td>0.014922</td>\n",
       "      <td>-0.033768</td>\n",
       "      <td>0.032345</td>\n",
       "      <td>-0.009779</td>\n",
       "      <td>0.021077</td>\n",
       "      <td>0.022465</td>\n",
       "      <td>-0.021588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7505</th>\n",
       "      <td>0.093012</td>\n",
       "      <td>-0.015717</td>\n",
       "      <td>-0.027347</td>\n",
       "      <td>0.021229</td>\n",
       "      <td>0.017567</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>-0.029287</td>\n",
       "      <td>0.026982</td>\n",
       "      <td>0.045942</td>\n",
       "      <td>-0.002156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030717</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>-0.012877</td>\n",
       "      <td>0.010124</td>\n",
       "      <td>-0.034091</td>\n",
       "      <td>-0.078648</td>\n",
       "      <td>-0.001790</td>\n",
       "      <td>0.029020</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>-0.041205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9324 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "7214  0.051633 -0.008142 -0.024332  0.013864  0.044475  0.006016 -0.052417   \n",
       "5884  0.034698 -0.087794 -0.008092 -0.039379  0.028995 -0.021274  0.029689   \n",
       "5651  0.064077  0.007670 -0.008914  0.016467  0.039864  0.021567 -0.062220   \n",
       "6812  0.048182 -0.069188 -0.023972  0.010279  0.016139 -0.006851  0.008167   \n",
       "3669  0.066079 -0.050984 -0.029105  0.020741  0.018208  0.018970 -0.063394   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1644  0.071723 -0.022951 -0.009669 -0.008356  0.000957  0.000765 -0.038034   \n",
       "677   0.087692 -0.021290 -0.025321  0.048477 -0.008885  0.007916 -0.019462   \n",
       "7800  0.078633 -0.008377 -0.026783  0.014870  0.009892  0.000110 -0.018558   \n",
       "6499  0.057288 -0.011589 -0.008272 -0.017983  0.037853  0.019753 -0.054403   \n",
       "7505  0.093012 -0.015717 -0.027347  0.021229  0.017567  0.003263 -0.029287   \n",
       "\n",
       "             7         8         9  ...       758       759       760  \\\n",
       "7214  0.025871  0.068986 -0.006843  ... -0.015038 -0.021133  0.009711   \n",
       "5884 -0.011294  0.052413 -0.041411  ... -0.034887 -0.030786 -0.015544   \n",
       "5651  0.032777  0.083319 -0.017697  ...  0.007360 -0.026584  0.026626   \n",
       "6812  0.008073  0.069693  0.034364  ... -0.030912 -0.047714 -0.040926   \n",
       "3669  0.043426  0.030005 -0.012341  ... -0.005794 -0.032436  0.021301   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1644  0.035612  0.069025 -0.024401  ... -0.001634 -0.023546 -0.013278   \n",
       "677  -0.002037  0.098750 -0.030091  ...  0.031360 -0.010586  0.033286   \n",
       "7800 -0.002488  0.052191  0.012958  ... -0.014843 -0.004679  0.010054   \n",
       "6499  0.052510  0.063096 -0.023402  ... -0.006385 -0.046968 -0.027560   \n",
       "7505  0.026982  0.045942 -0.002156  ... -0.030717  0.004115 -0.012877   \n",
       "\n",
       "           761       762       763       764       765       766       767  \n",
       "7214 -0.009949 -0.055740 -0.029697 -0.020551  0.041049  0.016886 -0.029579  \n",
       "5884  0.008737 -0.049981 -0.103033  0.019641  0.009213  0.016622 -0.023977  \n",
       "5651  0.012260 -0.066954 -0.048784 -0.034600  0.021107  0.030552 -0.021320  \n",
       "6812 -0.005258 -0.028718  0.014025  0.019760  0.024238 -0.020872 -0.040295  \n",
       "3669  0.029041 -0.003624 -0.006724  0.030850  0.007316 -0.011735 -0.015874  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1644 -0.001090 -0.058875 -0.088182  0.001628  0.027868  0.030174 -0.016046  \n",
       "677   0.023939 -0.062817 -0.037413 -0.013525  0.032097 -0.005044 -0.027736  \n",
       "7800  0.007563 -0.040454 -0.079680  0.025330  0.023660  0.022532 -0.028357  \n",
       "6499  0.014922 -0.033768  0.032345 -0.009779  0.021077  0.022465 -0.021588  \n",
       "7505  0.010124 -0.034091 -0.078648 -0.001790  0.029020 -0.000820 -0.041205  \n",
       "\n",
       "[9324 rows x 768 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9324, 768)\n",
      "(9324, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"/Users/withmocha/Desktop/DATA/Capston Design(2024)/data(sentence)/0426/test/sentence 5/test_data_after_vector.csv\",index_col=0)\n",
    "\n",
    "test_y=pd.DataFrame(data=test['predict'],columns=['predict'])\n",
    "test_x=test.drop(columns=['predict'])\n",
    "test_x=test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.051005</td>\n",
       "      <td>-0.047118</td>\n",
       "      <td>-0.002921</td>\n",
       "      <td>-0.011548</td>\n",
       "      <td>0.024685</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>-0.025701</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.068799</td>\n",
       "      <td>-0.030103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024075</td>\n",
       "      <td>-0.034789</td>\n",
       "      <td>-0.005816</td>\n",
       "      <td>0.014436</td>\n",
       "      <td>-0.046915</td>\n",
       "      <td>-0.061301</td>\n",
       "      <td>0.007005</td>\n",
       "      <td>0.013859</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>-0.031243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.071160</td>\n",
       "      <td>-0.060565</td>\n",
       "      <td>-0.031358</td>\n",
       "      <td>-0.003145</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>-0.003359</td>\n",
       "      <td>-0.016509</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>0.044384</td>\n",
       "      <td>-0.022227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002994</td>\n",
       "      <td>-0.014539</td>\n",
       "      <td>-0.015582</td>\n",
       "      <td>0.027477</td>\n",
       "      <td>-0.046895</td>\n",
       "      <td>-0.051260</td>\n",
       "      <td>-0.009035</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>0.016560</td>\n",
       "      <td>-0.014965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.036727</td>\n",
       "      <td>-0.092268</td>\n",
       "      <td>-0.029025</td>\n",
       "      <td>-0.020419</td>\n",
       "      <td>-0.009119</td>\n",
       "      <td>-0.006383</td>\n",
       "      <td>0.013706</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>0.051389</td>\n",
       "      <td>-0.006693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050692</td>\n",
       "      <td>-0.057042</td>\n",
       "      <td>-0.028925</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>-0.025570</td>\n",
       "      <td>-0.031317</td>\n",
       "      <td>0.026828</td>\n",
       "      <td>0.015227</td>\n",
       "      <td>0.008083</td>\n",
       "      <td>-0.010104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.025728</td>\n",
       "      <td>-0.010372</td>\n",
       "      <td>-0.011362</td>\n",
       "      <td>0.017628</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>-0.002340</td>\n",
       "      <td>-0.073366</td>\n",
       "      <td>0.038275</td>\n",
       "      <td>0.060411</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031430</td>\n",
       "      <td>-0.023008</td>\n",
       "      <td>0.029799</td>\n",
       "      <td>-0.035012</td>\n",
       "      <td>-0.049884</td>\n",
       "      <td>-0.087649</td>\n",
       "      <td>0.012382</td>\n",
       "      <td>0.012269</td>\n",
       "      <td>0.037782</td>\n",
       "      <td>-0.033409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.004861</td>\n",
       "      <td>0.038017</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.035669</td>\n",
       "      <td>0.051358</td>\n",
       "      <td>0.022744</td>\n",
       "      <td>-0.041680</td>\n",
       "      <td>0.074081</td>\n",
       "      <td>0.021195</td>\n",
       "      <td>0.028226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044995</td>\n",
       "      <td>0.029893</td>\n",
       "      <td>0.060972</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>-0.045003</td>\n",
       "      <td>0.006686</td>\n",
       "      <td>-0.064507</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>-0.041283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.053312</td>\n",
       "      <td>-0.055629</td>\n",
       "      <td>-0.026934</td>\n",
       "      <td>-0.025095</td>\n",
       "      <td>0.018797</td>\n",
       "      <td>-0.033329</td>\n",
       "      <td>-0.007243</td>\n",
       "      <td>-0.014299</td>\n",
       "      <td>0.020770</td>\n",
       "      <td>-0.005651</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008070</td>\n",
       "      <td>-0.049167</td>\n",
       "      <td>-0.043102</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>-0.029630</td>\n",
       "      <td>-0.071347</td>\n",
       "      <td>0.030696</td>\n",
       "      <td>-0.008128</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>-0.008206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.027508</td>\n",
       "      <td>-0.099843</td>\n",
       "      <td>-0.022063</td>\n",
       "      <td>-0.004257</td>\n",
       "      <td>-0.007561</td>\n",
       "      <td>-0.008260</td>\n",
       "      <td>0.008378</td>\n",
       "      <td>-0.006707</td>\n",
       "      <td>0.051894</td>\n",
       "      <td>-0.008191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050221</td>\n",
       "      <td>-0.040798</td>\n",
       "      <td>-0.024559</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>-0.030050</td>\n",
       "      <td>-0.043665</td>\n",
       "      <td>0.017916</td>\n",
       "      <td>0.019630</td>\n",
       "      <td>-0.003704</td>\n",
       "      <td>-0.020411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.064412</td>\n",
       "      <td>-0.038792</td>\n",
       "      <td>-0.037596</td>\n",
       "      <td>-0.001950</td>\n",
       "      <td>-0.014613</td>\n",
       "      <td>0.011568</td>\n",
       "      <td>-0.016508</td>\n",
       "      <td>0.046802</td>\n",
       "      <td>0.079677</td>\n",
       "      <td>-0.003892</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010047</td>\n",
       "      <td>-0.006434</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.006375</td>\n",
       "      <td>-0.043588</td>\n",
       "      <td>-0.046211</td>\n",
       "      <td>0.020631</td>\n",
       "      <td>0.033810</td>\n",
       "      <td>0.014317</td>\n",
       "      <td>-0.030672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.020202</td>\n",
       "      <td>-0.037717</td>\n",
       "      <td>-0.015586</td>\n",
       "      <td>-0.004426</td>\n",
       "      <td>0.025631</td>\n",
       "      <td>-0.003150</td>\n",
       "      <td>-0.035590</td>\n",
       "      <td>0.019095</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>-0.002850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043345</td>\n",
       "      <td>-0.052886</td>\n",
       "      <td>-0.045146</td>\n",
       "      <td>-0.021866</td>\n",
       "      <td>-0.045538</td>\n",
       "      <td>-0.043052</td>\n",
       "      <td>0.035637</td>\n",
       "      <td>0.016345</td>\n",
       "      <td>0.014842</td>\n",
       "      <td>-0.021804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.053041</td>\n",
       "      <td>-0.031516</td>\n",
       "      <td>-0.025086</td>\n",
       "      <td>-0.024722</td>\n",
       "      <td>0.010602</td>\n",
       "      <td>-0.005724</td>\n",
       "      <td>0.005407</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.032957</td>\n",
       "      <td>-0.028714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035848</td>\n",
       "      <td>-0.022893</td>\n",
       "      <td>-0.047753</td>\n",
       "      <td>0.021931</td>\n",
       "      <td>-0.024610</td>\n",
       "      <td>-0.054077</td>\n",
       "      <td>0.016625</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>0.014965</td>\n",
       "      <td>-0.014573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "145  0.051005 -0.047118 -0.002921 -0.011548  0.024685  0.003444 -0.025701   \n",
       "278  0.071160 -0.060565 -0.031358 -0.003145  0.012579 -0.003359 -0.016509   \n",
       "175  0.036727 -0.092268 -0.029025 -0.020419 -0.009119 -0.006383  0.013706   \n",
       "256  0.025728 -0.010372 -0.011362  0.017628  0.042218 -0.002340 -0.073366   \n",
       "190  0.004861  0.038017  0.005488  0.035669  0.051358  0.022744 -0.041680   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "138  0.053312 -0.055629 -0.026934 -0.025095  0.018797 -0.033329 -0.007243   \n",
       "74   0.027508 -0.099843 -0.022063 -0.004257 -0.007561 -0.008260  0.008378   \n",
       "164  0.064412 -0.038792 -0.037596 -0.001950 -0.014613  0.011568 -0.016508   \n",
       "123  0.020202 -0.037717 -0.015586 -0.004426  0.025631 -0.003150 -0.035590   \n",
       "70   0.053041 -0.031516 -0.025086 -0.024722  0.010602 -0.005724  0.005407   \n",
       "\n",
       "            7         8         9  ...       758       759       760  \\\n",
       "145  0.002052  0.068799 -0.030103  ... -0.024075 -0.034789 -0.005816   \n",
       "278  0.009185  0.044384 -0.022227  ... -0.002994 -0.014539 -0.015582   \n",
       "175 -0.001080  0.051389 -0.006693  ... -0.050692 -0.057042 -0.028925   \n",
       "256  0.038275  0.060411  0.004729  ... -0.031430 -0.023008  0.029799   \n",
       "190  0.074081  0.021195  0.028226  ... -0.044995  0.029893  0.060972   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "138 -0.014299  0.020770 -0.005651  ... -0.008070 -0.049167 -0.043102   \n",
       "74  -0.006707  0.051894 -0.008191  ... -0.050221 -0.040798 -0.024559   \n",
       "164  0.046802  0.079677 -0.003892  ... -0.010047 -0.006434  0.003459   \n",
       "123  0.019095  0.000859 -0.002850  ... -0.043345 -0.052886 -0.045146   \n",
       "70   0.002552  0.032957 -0.028714  ... -0.035848 -0.022893 -0.047753   \n",
       "\n",
       "          761       762       763       764       765       766       767  \n",
       "145  0.014436 -0.046915 -0.061301  0.007005  0.013859  0.007628 -0.031243  \n",
       "278  0.027477 -0.046895 -0.051260 -0.009035  0.012350  0.016560 -0.014965  \n",
       "175  0.005412 -0.025570 -0.031317  0.026828  0.015227  0.008083 -0.010104  \n",
       "256 -0.035012 -0.049884 -0.087649  0.012382  0.012269  0.037782 -0.033409  \n",
       "190  0.000272 -0.045003  0.006686 -0.064507  0.011445 -0.000819 -0.041283  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "138  0.003149 -0.029630 -0.071347  0.030696 -0.008128  0.004239 -0.008206  \n",
       "74   0.012560 -0.030050 -0.043665  0.017916  0.019630 -0.003704 -0.020411  \n",
       "164  0.006375 -0.043588 -0.046211  0.020631  0.033810  0.014317 -0.030672  \n",
       "123 -0.021866 -0.045538 -0.043052  0.035637  0.016345  0.014842 -0.021804  \n",
       "70   0.021931 -0.024610 -0.054077  0.016625  0.026365  0.014965 -0.014573  \n",
       "\n",
       "[300 rows x 768 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     predict\n",
       "145        0\n",
       "278        1\n",
       "175        0\n",
       "256        0\n",
       "190        0\n",
       "..       ...\n",
       "138        0\n",
       "74         0\n",
       "164        1\n",
       "123        0\n",
       "70         2\n",
       "\n",
       "[300 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler()\n",
    "\n",
    "#x_train = scaler.fit_transform(x_train)\n",
    "#x_test = scaler.transform(x_test) # test set에는 transform만 사용하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7214</th>\n",
       "      <td>0.051633</td>\n",
       "      <td>-0.008142</td>\n",
       "      <td>-0.024332</td>\n",
       "      <td>0.013864</td>\n",
       "      <td>0.044475</td>\n",
       "      <td>0.006016</td>\n",
       "      <td>-0.052417</td>\n",
       "      <td>0.025871</td>\n",
       "      <td>0.068986</td>\n",
       "      <td>-0.006843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015038</td>\n",
       "      <td>-0.021133</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>-0.009949</td>\n",
       "      <td>-0.055740</td>\n",
       "      <td>-0.029697</td>\n",
       "      <td>-0.020551</td>\n",
       "      <td>0.041049</td>\n",
       "      <td>0.016886</td>\n",
       "      <td>-0.029579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>0.034698</td>\n",
       "      <td>-0.087794</td>\n",
       "      <td>-0.008092</td>\n",
       "      <td>-0.039379</td>\n",
       "      <td>0.028995</td>\n",
       "      <td>-0.021274</td>\n",
       "      <td>0.029689</td>\n",
       "      <td>-0.011294</td>\n",
       "      <td>0.052413</td>\n",
       "      <td>-0.041411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034887</td>\n",
       "      <td>-0.030786</td>\n",
       "      <td>-0.015544</td>\n",
       "      <td>0.008737</td>\n",
       "      <td>-0.049981</td>\n",
       "      <td>-0.103033</td>\n",
       "      <td>0.019641</td>\n",
       "      <td>0.009213</td>\n",
       "      <td>0.016622</td>\n",
       "      <td>-0.023977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>0.064077</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>-0.008914</td>\n",
       "      <td>0.016467</td>\n",
       "      <td>0.039864</td>\n",
       "      <td>0.021567</td>\n",
       "      <td>-0.062220</td>\n",
       "      <td>0.032777</td>\n",
       "      <td>0.083319</td>\n",
       "      <td>-0.017697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007360</td>\n",
       "      <td>-0.026584</td>\n",
       "      <td>0.026626</td>\n",
       "      <td>0.012260</td>\n",
       "      <td>-0.066954</td>\n",
       "      <td>-0.048784</td>\n",
       "      <td>-0.034600</td>\n",
       "      <td>0.021107</td>\n",
       "      <td>0.030552</td>\n",
       "      <td>-0.021320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6812</th>\n",
       "      <td>0.048182</td>\n",
       "      <td>-0.069188</td>\n",
       "      <td>-0.023972</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.016139</td>\n",
       "      <td>-0.006851</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.008073</td>\n",
       "      <td>0.069693</td>\n",
       "      <td>0.034364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030912</td>\n",
       "      <td>-0.047714</td>\n",
       "      <td>-0.040926</td>\n",
       "      <td>-0.005258</td>\n",
       "      <td>-0.028718</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>0.019760</td>\n",
       "      <td>0.024238</td>\n",
       "      <td>-0.020872</td>\n",
       "      <td>-0.040295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>0.066079</td>\n",
       "      <td>-0.050984</td>\n",
       "      <td>-0.029105</td>\n",
       "      <td>0.020741</td>\n",
       "      <td>0.018208</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>-0.063394</td>\n",
       "      <td>0.043426</td>\n",
       "      <td>0.030005</td>\n",
       "      <td>-0.012341</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005794</td>\n",
       "      <td>-0.032436</td>\n",
       "      <td>0.021301</td>\n",
       "      <td>0.029041</td>\n",
       "      <td>-0.003624</td>\n",
       "      <td>-0.006724</td>\n",
       "      <td>0.030850</td>\n",
       "      <td>0.007316</td>\n",
       "      <td>-0.011735</td>\n",
       "      <td>-0.015874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>0.071723</td>\n",
       "      <td>-0.022951</td>\n",
       "      <td>-0.009669</td>\n",
       "      <td>-0.008356</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>-0.038034</td>\n",
       "      <td>0.035612</td>\n",
       "      <td>0.069025</td>\n",
       "      <td>-0.024401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001634</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013278</td>\n",
       "      <td>-0.001090</td>\n",
       "      <td>-0.058875</td>\n",
       "      <td>-0.088182</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.027868</td>\n",
       "      <td>0.030174</td>\n",
       "      <td>-0.016046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>0.087692</td>\n",
       "      <td>-0.021290</td>\n",
       "      <td>-0.025321</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>-0.008885</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>-0.019462</td>\n",
       "      <td>-0.002037</td>\n",
       "      <td>0.098750</td>\n",
       "      <td>-0.030091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031360</td>\n",
       "      <td>-0.010586</td>\n",
       "      <td>0.033286</td>\n",
       "      <td>0.023939</td>\n",
       "      <td>-0.062817</td>\n",
       "      <td>-0.037413</td>\n",
       "      <td>-0.013525</td>\n",
       "      <td>0.032097</td>\n",
       "      <td>-0.005044</td>\n",
       "      <td>-0.027736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>0.078633</td>\n",
       "      <td>-0.008377</td>\n",
       "      <td>-0.026783</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.018558</td>\n",
       "      <td>-0.002488</td>\n",
       "      <td>0.052191</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014843</td>\n",
       "      <td>-0.004679</td>\n",
       "      <td>0.010054</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>-0.040454</td>\n",
       "      <td>-0.079680</td>\n",
       "      <td>0.025330</td>\n",
       "      <td>0.023660</td>\n",
       "      <td>0.022532</td>\n",
       "      <td>-0.028357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6499</th>\n",
       "      <td>0.057288</td>\n",
       "      <td>-0.011589</td>\n",
       "      <td>-0.008272</td>\n",
       "      <td>-0.017983</td>\n",
       "      <td>0.037853</td>\n",
       "      <td>0.019753</td>\n",
       "      <td>-0.054403</td>\n",
       "      <td>0.052510</td>\n",
       "      <td>0.063096</td>\n",
       "      <td>-0.023402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006385</td>\n",
       "      <td>-0.046968</td>\n",
       "      <td>-0.027560</td>\n",
       "      <td>0.014922</td>\n",
       "      <td>-0.033768</td>\n",
       "      <td>0.032345</td>\n",
       "      <td>-0.009779</td>\n",
       "      <td>0.021077</td>\n",
       "      <td>0.022465</td>\n",
       "      <td>-0.021588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7505</th>\n",
       "      <td>0.093012</td>\n",
       "      <td>-0.015717</td>\n",
       "      <td>-0.027347</td>\n",
       "      <td>0.021229</td>\n",
       "      <td>0.017567</td>\n",
       "      <td>0.003263</td>\n",
       "      <td>-0.029287</td>\n",
       "      <td>0.026982</td>\n",
       "      <td>0.045942</td>\n",
       "      <td>-0.002156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030717</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>-0.012877</td>\n",
       "      <td>0.010124</td>\n",
       "      <td>-0.034091</td>\n",
       "      <td>-0.078648</td>\n",
       "      <td>-0.001790</td>\n",
       "      <td>0.029020</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>-0.041205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9324 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "7214  0.051633 -0.008142 -0.024332  0.013864  0.044475  0.006016 -0.052417   \n",
       "5884  0.034698 -0.087794 -0.008092 -0.039379  0.028995 -0.021274  0.029689   \n",
       "5651  0.064077  0.007670 -0.008914  0.016467  0.039864  0.021567 -0.062220   \n",
       "6812  0.048182 -0.069188 -0.023972  0.010279  0.016139 -0.006851  0.008167   \n",
       "3669  0.066079 -0.050984 -0.029105  0.020741  0.018208  0.018970 -0.063394   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1644  0.071723 -0.022951 -0.009669 -0.008356  0.000957  0.000765 -0.038034   \n",
       "677   0.087692 -0.021290 -0.025321  0.048477 -0.008885  0.007916 -0.019462   \n",
       "7800  0.078633 -0.008377 -0.026783  0.014870  0.009892  0.000110 -0.018558   \n",
       "6499  0.057288 -0.011589 -0.008272 -0.017983  0.037853  0.019753 -0.054403   \n",
       "7505  0.093012 -0.015717 -0.027347  0.021229  0.017567  0.003263 -0.029287   \n",
       "\n",
       "             7         8         9  ...       758       759       760  \\\n",
       "7214  0.025871  0.068986 -0.006843  ... -0.015038 -0.021133  0.009711   \n",
       "5884 -0.011294  0.052413 -0.041411  ... -0.034887 -0.030786 -0.015544   \n",
       "5651  0.032777  0.083319 -0.017697  ...  0.007360 -0.026584  0.026626   \n",
       "6812  0.008073  0.069693  0.034364  ... -0.030912 -0.047714 -0.040926   \n",
       "3669  0.043426  0.030005 -0.012341  ... -0.005794 -0.032436  0.021301   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1644  0.035612  0.069025 -0.024401  ... -0.001634 -0.023546 -0.013278   \n",
       "677  -0.002037  0.098750 -0.030091  ...  0.031360 -0.010586  0.033286   \n",
       "7800 -0.002488  0.052191  0.012958  ... -0.014843 -0.004679  0.010054   \n",
       "6499  0.052510  0.063096 -0.023402  ... -0.006385 -0.046968 -0.027560   \n",
       "7505  0.026982  0.045942 -0.002156  ... -0.030717  0.004115 -0.012877   \n",
       "\n",
       "           761       762       763       764       765       766       767  \n",
       "7214 -0.009949 -0.055740 -0.029697 -0.020551  0.041049  0.016886 -0.029579  \n",
       "5884  0.008737 -0.049981 -0.103033  0.019641  0.009213  0.016622 -0.023977  \n",
       "5651  0.012260 -0.066954 -0.048784 -0.034600  0.021107  0.030552 -0.021320  \n",
       "6812 -0.005258 -0.028718  0.014025  0.019760  0.024238 -0.020872 -0.040295  \n",
       "3669  0.029041 -0.003624 -0.006724  0.030850  0.007316 -0.011735 -0.015874  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1644 -0.001090 -0.058875 -0.088182  0.001628  0.027868  0.030174 -0.016046  \n",
       "677   0.023939 -0.062817 -0.037413 -0.013525  0.032097 -0.005044 -0.027736  \n",
       "7800  0.007563 -0.040454 -0.079680  0.025330  0.023660  0.022532 -0.028357  \n",
       "6499  0.014922 -0.033768  0.032345 -0.009779  0.021077  0.022465 -0.021588  \n",
       "7505  0.010124 -0.034091 -0.078648 -0.001790  0.029020 -0.000820 -0.041205  \n",
       "\n",
       "[9324 rows x 768 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train=to_categorical(y_train,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변경된 차원 정보 : (9324, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))\n",
    "test_x = tf.reshape(test_x,(test_x.shape[0],test_x.shape[1],1))\n",
    "print('변경된 차원 정보 :',x_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9324, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9324, 768, 1), dtype=float64, numpy=\n",
       "array([[[ 0.05163314],\n",
       "        [-0.00814165],\n",
       "        [-0.02433206],\n",
       "        ...,\n",
       "        [ 0.04104915],\n",
       "        [ 0.01688556],\n",
       "        [-0.02957866]],\n",
       "\n",
       "       [[ 0.03469766],\n",
       "        [-0.08779408],\n",
       "        [-0.00809169],\n",
       "        ...,\n",
       "        [ 0.00921288],\n",
       "        [ 0.01662171],\n",
       "        [-0.02397713]],\n",
       "\n",
       "       [[ 0.06407749],\n",
       "        [ 0.00766974],\n",
       "        [-0.008914  ],\n",
       "        ...,\n",
       "        [ 0.02110684],\n",
       "        [ 0.0305524 ],\n",
       "        [-0.02132013]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.07863324],\n",
       "        [-0.00837698],\n",
       "        [-0.02678296],\n",
       "        ...,\n",
       "        [ 0.02366043],\n",
       "        [ 0.0225317 ],\n",
       "        [-0.02835702]],\n",
       "\n",
       "       [[ 0.05728787],\n",
       "        [-0.01158941],\n",
       "        [-0.00827212],\n",
       "        ...,\n",
       "        [ 0.02107701],\n",
       "        [ 0.02246503],\n",
       "        [-0.02158763]],\n",
       "\n",
       "       [[ 0.09301195],\n",
       "        [-0.01571734],\n",
       "        [-0.02734674],\n",
       "        ...,\n",
       "        [ 0.02901995],\n",
       "        [-0.00082002],\n",
       "        [-0.04120519]]])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 768, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# ReduceLROnPlateau 콜백 생성\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', # 모니터링 대상 성능 지표\n",
    "                              factor=0.95,        # 학습률 감소 비율\n",
    "                              patience=10,        # 성능 향상을 기다리는 에폭 수\n",
    "                              verbose=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/conda_cpu/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">766</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">764</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">762</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,552</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">381</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">381</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">379</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,568</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">377</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,552</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">188</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">188</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">188</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,472</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">188</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">41,216</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,368</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m766\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m764\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m762\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,552\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m381\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m381\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m379\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m1,568\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m377\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,552\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m188\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m188\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m188\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m41,472\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m188\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m41,216\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m10,368\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,795</span> (413.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m105,795\u001b[0m (413.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">105,795</span> (413.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m105,795\u001b[0m (413.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.activations import elu\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "# 입력층 - 1D CNN\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='elu', input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='elu'))\n",
    "model.add(Conv1D(filters=16, kernel_size=3, activation='elu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "# 추가 CNN 층\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='elu'))\n",
    "model.add(Conv1D(filters=16, kernel_size=3, activation='elu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "# 중간층 - LSTM\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(16, return_sequences=False)))\n",
    "model.add(Dropout(0.5))\n",
    "# 추가 Dense 층\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(Dense(16, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 출력층\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# 최적화 알고리즘과 손실 함수 설정\n",
    "model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=0.002,weight_decay=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early Stopping과 Model Checkpoint 적용\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True,verbose=1,mode='min')\n",
    "model_checkpoint = ModelCheckpoint('best_weights.keras', monitor='val_accuracy',save_best_only=True,verbose=1,mode='max',save_weights_only=False)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, sleep\n",
    "\n",
    "start = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816ms/step - accuracy: 0.5396 - loss: 1.0058\n",
      "Epoch 1: val_accuracy improved from -inf to 0.61662, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 913ms/step - accuracy: 0.5419 - loss: 1.0036 - val_accuracy: 0.6166 - val_loss: 0.9081 - learning_rate: 0.0020\n",
      "Epoch 2/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788ms/step - accuracy: 0.6172 - loss: 0.9240\n",
      "Epoch 2: val_accuracy did not improve from 0.61662\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 856ms/step - accuracy: 0.6173 - loss: 0.9237 - val_accuracy: 0.6166 - val_loss: 0.9059 - learning_rate: 0.0020\n",
      "Epoch 3/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768ms/step - accuracy: 0.6288 - loss: 0.9080\n",
      "Epoch 3: val_accuracy did not improve from 0.61662\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 845ms/step - accuracy: 0.6284 - loss: 0.9081 - val_accuracy: 0.6166 - val_loss: 0.8974 - learning_rate: 0.0020\n",
      "Epoch 4/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 737ms/step - accuracy: 0.6284 - loss: 0.8833\n",
      "Epoch 4: val_accuracy improved from 0.61662 to 0.74048, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 811ms/step - accuracy: 0.6295 - loss: 0.8808 - val_accuracy: 0.7405 - val_loss: 0.6535 - learning_rate: 0.0020\n",
      "Epoch 5/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788ms/step - accuracy: 0.7038 - loss: 0.7187\n",
      "Epoch 5: val_accuracy improved from 0.74048 to 0.74745, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 857ms/step - accuracy: 0.7040 - loss: 0.7186 - val_accuracy: 0.7475 - val_loss: 0.6136 - learning_rate: 0.0020\n",
      "Epoch 6/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818ms/step - accuracy: 0.7505 - loss: 0.6643\n",
      "Epoch 6: val_accuracy improved from 0.74745 to 0.78767, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 887ms/step - accuracy: 0.7508 - loss: 0.6633 - val_accuracy: 0.7877 - val_loss: 0.5372 - learning_rate: 0.0020\n",
      "Epoch 7/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699ms/step - accuracy: 0.7473 - loss: 0.6479\n",
      "Epoch 7: val_accuracy improved from 0.78767 to 0.79303, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 775ms/step - accuracy: 0.7473 - loss: 0.6480 - val_accuracy: 0.7930 - val_loss: 0.5270 - learning_rate: 0.0020\n",
      "Epoch 8/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693ms/step - accuracy: 0.7691 - loss: 0.5995\n",
      "Epoch 8: val_accuracy improved from 0.79303 to 0.80054, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 761ms/step - accuracy: 0.7689 - loss: 0.5996 - val_accuracy: 0.8005 - val_loss: 0.4942 - learning_rate: 0.0020\n",
      "Epoch 9/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 680ms/step - accuracy: 0.7885 - loss: 0.5697\n",
      "Epoch 9: val_accuracy improved from 0.80054 to 0.80429, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 749ms/step - accuracy: 0.7884 - loss: 0.5697 - val_accuracy: 0.8043 - val_loss: 0.4802 - learning_rate: 0.0020\n",
      "Epoch 10/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690ms/step - accuracy: 0.7874 - loss: 0.5484\n",
      "Epoch 10: val_accuracy did not improve from 0.80429\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 759ms/step - accuracy: 0.7875 - loss: 0.5483 - val_accuracy: 0.8043 - val_loss: 0.4760 - learning_rate: 0.0020\n",
      "Epoch 11/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 689ms/step - accuracy: 0.7832 - loss: 0.5598\n",
      "Epoch 11: val_accuracy improved from 0.80429 to 0.81930, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 754ms/step - accuracy: 0.7835 - loss: 0.5589 - val_accuracy: 0.8193 - val_loss: 0.4342 - learning_rate: 0.0020\n",
      "Epoch 12/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693ms/step - accuracy: 0.7961 - loss: 0.5411\n",
      "Epoch 12: val_accuracy did not improve from 0.81930\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 764ms/step - accuracy: 0.7963 - loss: 0.5404 - val_accuracy: 0.8064 - val_loss: 0.4636 - learning_rate: 0.0020\n",
      "Epoch 13/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691ms/step - accuracy: 0.8011 - loss: 0.5090\n",
      "Epoch 13: val_accuracy improved from 0.81930 to 0.82520, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 759ms/step - accuracy: 0.8011 - loss: 0.5088 - val_accuracy: 0.8252 - val_loss: 0.4066 - learning_rate: 0.0020\n",
      "Epoch 14/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713ms/step - accuracy: 0.8087 - loss: 0.4907\n",
      "Epoch 14: val_accuracy improved from 0.82520 to 0.82949, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 783ms/step - accuracy: 0.8084 - loss: 0.4912 - val_accuracy: 0.8295 - val_loss: 0.3966 - learning_rate: 0.0020\n",
      "Epoch 15/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753ms/step - accuracy: 0.8082 - loss: 0.4791\n",
      "Epoch 15: val_accuracy improved from 0.82949 to 0.83807, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 825ms/step - accuracy: 0.8083 - loss: 0.4790 - val_accuracy: 0.8381 - val_loss: 0.3769 - learning_rate: 0.0020\n",
      "Epoch 16/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739ms/step - accuracy: 0.8195 - loss: 0.4580\n",
      "Epoch 16: val_accuracy did not improve from 0.83807\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 803ms/step - accuracy: 0.8193 - loss: 0.4587 - val_accuracy: 0.8311 - val_loss: 0.4015 - learning_rate: 0.0020\n",
      "Epoch 17/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 717ms/step - accuracy: 0.8146 - loss: 0.4686\n",
      "Epoch 17: val_accuracy did not improve from 0.83807\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 781ms/step - accuracy: 0.8146 - loss: 0.4684 - val_accuracy: 0.8273 - val_loss: 0.3828 - learning_rate: 0.0020\n",
      "Epoch 18/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724ms/step - accuracy: 0.8135 - loss: 0.4594\n",
      "Epoch 18: val_accuracy did not improve from 0.83807\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 793ms/step - accuracy: 0.8135 - loss: 0.4598 - val_accuracy: 0.8279 - val_loss: 0.3902 - learning_rate: 0.0020\n",
      "Epoch 19/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706ms/step - accuracy: 0.8156 - loss: 0.4552\n",
      "Epoch 19: val_accuracy did not improve from 0.83807\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 770ms/step - accuracy: 0.8155 - loss: 0.4556 - val_accuracy: 0.8343 - val_loss: 0.3679 - learning_rate: 0.0020\n",
      "Epoch 20/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 714ms/step - accuracy: 0.8233 - loss: 0.4399\n",
      "Epoch 20: val_accuracy improved from 0.83807 to 0.84075, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 785ms/step - accuracy: 0.8234 - loss: 0.4398 - val_accuracy: 0.8408 - val_loss: 0.3525 - learning_rate: 0.0020\n",
      "Epoch 21/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715ms/step - accuracy: 0.8185 - loss: 0.4421\n",
      "Epoch 21: val_accuracy did not improve from 0.84075\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 784ms/step - accuracy: 0.8185 - loss: 0.4421 - val_accuracy: 0.8311 - val_loss: 0.3797 - learning_rate: 0.0020\n",
      "Epoch 22/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691ms/step - accuracy: 0.8289 - loss: 0.4317\n",
      "Epoch 22: val_accuracy improved from 0.84075 to 0.84290, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 763ms/step - accuracy: 0.8284 - loss: 0.4321 - val_accuracy: 0.8429 - val_loss: 0.3581 - learning_rate: 0.0020\n",
      "Epoch 23/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716ms/step - accuracy: 0.8212 - loss: 0.4303\n",
      "Epoch 23: val_accuracy did not improve from 0.84290\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 791ms/step - accuracy: 0.8214 - loss: 0.4305 - val_accuracy: 0.8391 - val_loss: 0.3606 - learning_rate: 0.0020\n",
      "Epoch 24/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 699ms/step - accuracy: 0.8152 - loss: 0.4596\n",
      "Epoch 24: val_accuracy did not improve from 0.84290\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 776ms/step - accuracy: 0.8155 - loss: 0.4587 - val_accuracy: 0.8402 - val_loss: 0.3589 - learning_rate: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694ms/step - accuracy: 0.8302 - loss: 0.4208\n",
      "Epoch 25: val_accuracy did not improve from 0.84290\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 772ms/step - accuracy: 0.8300 - loss: 0.4211 - val_accuracy: 0.8386 - val_loss: 0.3440 - learning_rate: 0.0020\n",
      "Epoch 26/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 736ms/step - accuracy: 0.8289 - loss: 0.4199\n",
      "Epoch 26: val_accuracy did not improve from 0.84290\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 816ms/step - accuracy: 0.8288 - loss: 0.4203 - val_accuracy: 0.8408 - val_loss: 0.3439 - learning_rate: 0.0020\n",
      "Epoch 27/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907ms/step - accuracy: 0.8310 - loss: 0.4211\n",
      "Epoch 27: val_accuracy did not improve from 0.84290\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 989ms/step - accuracy: 0.8311 - loss: 0.4206 - val_accuracy: 0.8381 - val_loss: 0.3361 - learning_rate: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800ms/step - accuracy: 0.8265 - loss: 0.4060\n",
      "Epoch 28: val_accuracy improved from 0.84290 to 0.84450, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 882ms/step - accuracy: 0.8269 - loss: 0.4056 - val_accuracy: 0.8445 - val_loss: 0.3255 - learning_rate: 0.0020\n",
      "Epoch 29/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800ms/step - accuracy: 0.8317 - loss: 0.3945\n",
      "Epoch 29: val_accuracy did not improve from 0.84450\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 883ms/step - accuracy: 0.8316 - loss: 0.3950 - val_accuracy: 0.8434 - val_loss: 0.3400 - learning_rate: 0.0020\n",
      "Epoch 30/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 764ms/step - accuracy: 0.8401 - loss: 0.3921\n",
      "Epoch 30: val_accuracy did not improve from 0.84450\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 846ms/step - accuracy: 0.8398 - loss: 0.3922 - val_accuracy: 0.8440 - val_loss: 0.3170 - learning_rate: 0.0020\n",
      "Epoch 31/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768ms/step - accuracy: 0.8388 - loss: 0.3775\n",
      "Epoch 31: val_accuracy improved from 0.84450 to 0.85308, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 855ms/step - accuracy: 0.8387 - loss: 0.3779 - val_accuracy: 0.8531 - val_loss: 0.3169 - learning_rate: 0.0020\n",
      "Epoch 32/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792ms/step - accuracy: 0.8390 - loss: 0.3805\n",
      "Epoch 32: val_accuracy improved from 0.85308 to 0.85523, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 879ms/step - accuracy: 0.8391 - loss: 0.3807 - val_accuracy: 0.8552 - val_loss: 0.3149 - learning_rate: 0.0020\n",
      "Epoch 33/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 820ms/step - accuracy: 0.8358 - loss: 0.3821\n",
      "Epoch 33: val_accuracy improved from 0.85523 to 0.86434, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 910ms/step - accuracy: 0.8359 - loss: 0.3818 - val_accuracy: 0.8643 - val_loss: 0.3007 - learning_rate: 0.0020\n",
      "Epoch 34/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836ms/step - accuracy: 0.8370 - loss: 0.3773\n",
      "Epoch 34: val_accuracy did not improve from 0.86434\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 922ms/step - accuracy: 0.8371 - loss: 0.3777 - val_accuracy: 0.8558 - val_loss: 0.3118 - learning_rate: 0.0020\n",
      "Epoch 35/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794ms/step - accuracy: 0.8348 - loss: 0.3930\n",
      "Epoch 35: val_accuracy improved from 0.86434 to 0.86756, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 884ms/step - accuracy: 0.8352 - loss: 0.3924 - val_accuracy: 0.8676 - val_loss: 0.3017 - learning_rate: 0.0020\n",
      "Epoch 36/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821ms/step - accuracy: 0.8425 - loss: 0.3773\n",
      "Epoch 36: val_accuracy did not improve from 0.86756\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 904ms/step - accuracy: 0.8423 - loss: 0.3778 - val_accuracy: 0.8558 - val_loss: 0.3054 - learning_rate: 0.0020\n",
      "Epoch 37/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808ms/step - accuracy: 0.8398 - loss: 0.3716\n",
      "Epoch 37: val_accuracy did not improve from 0.86756\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 895ms/step - accuracy: 0.8399 - loss: 0.3719 - val_accuracy: 0.8504 - val_loss: 0.3154 - learning_rate: 0.0020\n",
      "Epoch 38/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816ms/step - accuracy: 0.8447 - loss: 0.3788\n",
      "Epoch 38: val_accuracy did not improve from 0.86756\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 907ms/step - accuracy: 0.8445 - loss: 0.3788 - val_accuracy: 0.8638 - val_loss: 0.2981 - learning_rate: 0.0020\n",
      "Epoch 39/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794ms/step - accuracy: 0.8463 - loss: 0.3503\n",
      "Epoch 39: val_accuracy improved from 0.86756 to 0.87078, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 881ms/step - accuracy: 0.8463 - loss: 0.3505 - val_accuracy: 0.8708 - val_loss: 0.3030 - learning_rate: 0.0020\n",
      "Epoch 40/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832ms/step - accuracy: 0.8508 - loss: 0.3620\n",
      "Epoch 40: val_accuracy did not improve from 0.87078\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 915ms/step - accuracy: 0.8507 - loss: 0.3621 - val_accuracy: 0.8627 - val_loss: 0.3128 - learning_rate: 0.0020\n",
      "Epoch 41/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799ms/step - accuracy: 0.8445 - loss: 0.3630\n",
      "Epoch 41: val_accuracy did not improve from 0.87078\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 891ms/step - accuracy: 0.8444 - loss: 0.3632 - val_accuracy: 0.8665 - val_loss: 0.2921 - learning_rate: 0.0020\n",
      "Epoch 42/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817ms/step - accuracy: 0.8528 - loss: 0.3531\n",
      "Epoch 42: val_accuracy did not improve from 0.87078\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 910ms/step - accuracy: 0.8527 - loss: 0.3530 - val_accuracy: 0.8686 - val_loss: 0.2990 - learning_rate: 0.0020\n",
      "Epoch 43/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801ms/step - accuracy: 0.8466 - loss: 0.3528\n",
      "Epoch 43: val_accuracy improved from 0.87078 to 0.87292, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 893ms/step - accuracy: 0.8465 - loss: 0.3533 - val_accuracy: 0.8729 - val_loss: 0.2960 - learning_rate: 0.0020\n",
      "Epoch 44/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872ms/step - accuracy: 0.8494 - loss: 0.3605\n",
      "Epoch 44: val_accuracy improved from 0.87292 to 0.87346, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 964ms/step - accuracy: 0.8496 - loss: 0.3599 - val_accuracy: 0.8735 - val_loss: 0.2865 - learning_rate: 0.0020\n",
      "Epoch 45/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861ms/step - accuracy: 0.8566 - loss: 0.3447\n",
      "Epoch 45: val_accuracy improved from 0.87346 to 0.87399, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 954ms/step - accuracy: 0.8564 - loss: 0.3449 - val_accuracy: 0.8740 - val_loss: 0.2875 - learning_rate: 0.0020\n",
      "Epoch 46/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843ms/step - accuracy: 0.8490 - loss: 0.3513\n",
      "Epoch 46: val_accuracy improved from 0.87399 to 0.87560, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 933ms/step - accuracy: 0.8492 - loss: 0.3509 - val_accuracy: 0.8756 - val_loss: 0.2805 - learning_rate: 0.0020\n",
      "Epoch 47/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 839ms/step - accuracy: 0.8600 - loss: 0.3286\n",
      "Epoch 47: val_accuracy improved from 0.87560 to 0.87614, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 932ms/step - accuracy: 0.8597 - loss: 0.3291 - val_accuracy: 0.8761 - val_loss: 0.2811 - learning_rate: 0.0020\n",
      "Epoch 48/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825ms/step - accuracy: 0.8583 - loss: 0.3558\n",
      "Epoch 48: val_accuracy improved from 0.87614 to 0.88043, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 912ms/step - accuracy: 0.8583 - loss: 0.3554 - val_accuracy: 0.8804 - val_loss: 0.2736 - learning_rate: 0.0020\n",
      "Epoch 49/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850ms/step - accuracy: 0.8605 - loss: 0.3363\n",
      "Epoch 49: val_accuracy did not improve from 0.88043\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 944ms/step - accuracy: 0.8603 - loss: 0.3365 - val_accuracy: 0.8777 - val_loss: 0.2798 - learning_rate: 0.0020\n",
      "Epoch 50/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805ms/step - accuracy: 0.8559 - loss: 0.3493\n",
      "Epoch 50: val_accuracy improved from 0.88043 to 0.88097, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 896ms/step - accuracy: 0.8556 - loss: 0.3492 - val_accuracy: 0.8810 - val_loss: 0.2736 - learning_rate: 0.0020\n",
      "Epoch 51/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858ms/step - accuracy: 0.8616 - loss: 0.3239\n",
      "Epoch 51: val_accuracy did not improve from 0.88097\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 950ms/step - accuracy: 0.8616 - loss: 0.3243 - val_accuracy: 0.8783 - val_loss: 0.2693 - learning_rate: 0.0020\n",
      "Epoch 52/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833ms/step - accuracy: 0.8658 - loss: 0.3386\n",
      "Epoch 52: val_accuracy improved from 0.88097 to 0.88472, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 921ms/step - accuracy: 0.8657 - loss: 0.3382 - val_accuracy: 0.8847 - val_loss: 0.2535 - learning_rate: 0.0020\n",
      "Epoch 53/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912ms/step - accuracy: 0.8704 - loss: 0.3058\n",
      "Epoch 53: val_accuracy did not improve from 0.88472\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8703 - loss: 0.3063 - val_accuracy: 0.8820 - val_loss: 0.2575 - learning_rate: 0.0020\n",
      "Epoch 54/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829ms/step - accuracy: 0.8662 - loss: 0.3194\n",
      "Epoch 54: val_accuracy did not improve from 0.88472\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 918ms/step - accuracy: 0.8663 - loss: 0.3195 - val_accuracy: 0.8777 - val_loss: 0.2803 - learning_rate: 0.0020\n",
      "Epoch 55/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802ms/step - accuracy: 0.8567 - loss: 0.3499\n",
      "Epoch 55: val_accuracy improved from 0.88472 to 0.88633, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 894ms/step - accuracy: 0.8570 - loss: 0.3494 - val_accuracy: 0.8863 - val_loss: 0.2685 - learning_rate: 0.0020\n",
      "Epoch 56/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831ms/step - accuracy: 0.8627 - loss: 0.3268\n",
      "Epoch 56: val_accuracy did not improve from 0.88633\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 916ms/step - accuracy: 0.8628 - loss: 0.3266 - val_accuracy: 0.8863 - val_loss: 0.2694 - learning_rate: 0.0020\n",
      "Epoch 57/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763ms/step - accuracy: 0.8695 - loss: 0.3162\n",
      "Epoch 57: val_accuracy did not improve from 0.88633\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 847ms/step - accuracy: 0.8694 - loss: 0.3164 - val_accuracy: 0.8831 - val_loss: 0.2618 - learning_rate: 0.0020\n",
      "Epoch 58/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 763ms/step - accuracy: 0.8744 - loss: 0.3090\n",
      "Epoch 58: val_accuracy improved from 0.88633 to 0.89866, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 848ms/step - accuracy: 0.8743 - loss: 0.3089 - val_accuracy: 0.8987 - val_loss: 0.2525 - learning_rate: 0.0020\n",
      "Epoch 59/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799ms/step - accuracy: 0.8638 - loss: 0.3279\n",
      "Epoch 59: val_accuracy did not improve from 0.89866\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 885ms/step - accuracy: 0.8641 - loss: 0.3275 - val_accuracy: 0.8895 - val_loss: 0.2545 - learning_rate: 0.0020\n",
      "Epoch 60/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772ms/step - accuracy: 0.8715 - loss: 0.3077\n",
      "Epoch 60: val_accuracy did not improve from 0.89866\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 856ms/step - accuracy: 0.8711 - loss: 0.3083 - val_accuracy: 0.8949 - val_loss: 0.2399 - learning_rate: 0.0020\n",
      "Epoch 61/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784ms/step - accuracy: 0.8680 - loss: 0.3176\n",
      "Epoch 61: val_accuracy did not improve from 0.89866\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 872ms/step - accuracy: 0.8682 - loss: 0.3173 - val_accuracy: 0.8981 - val_loss: 0.2489 - learning_rate: 0.0020\n",
      "Epoch 62/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779ms/step - accuracy: 0.8698 - loss: 0.3096\n",
      "Epoch 62: val_accuracy did not improve from 0.89866\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 867ms/step - accuracy: 0.8698 - loss: 0.3095 - val_accuracy: 0.8949 - val_loss: 0.2500 - learning_rate: 0.0020\n",
      "Epoch 63/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767ms/step - accuracy: 0.8725 - loss: 0.2929\n",
      "Epoch 63: val_accuracy improved from 0.89866 to 0.89973, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 856ms/step - accuracy: 0.8725 - loss: 0.2931 - val_accuracy: 0.8997 - val_loss: 0.2503 - learning_rate: 0.0020\n",
      "Epoch 64/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 864ms/step - accuracy: 0.8736 - loss: 0.2966\n",
      "Epoch 64: val_accuracy did not improve from 0.89973\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 949ms/step - accuracy: 0.8736 - loss: 0.2966 - val_accuracy: 0.8944 - val_loss: 0.2521 - learning_rate: 0.0020\n",
      "Epoch 65/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776ms/step - accuracy: 0.8767 - loss: 0.3000\n",
      "Epoch 65: val_accuracy improved from 0.89973 to 0.90670, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 864ms/step - accuracy: 0.8766 - loss: 0.3001 - val_accuracy: 0.9067 - val_loss: 0.2191 - learning_rate: 0.0020\n",
      "Epoch 66/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804ms/step - accuracy: 0.8792 - loss: 0.2944\n",
      "Epoch 66: val_accuracy did not improve from 0.90670\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 892ms/step - accuracy: 0.8794 - loss: 0.2941 - val_accuracy: 0.9035 - val_loss: 0.2268 - learning_rate: 0.0020\n",
      "Epoch 67/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 768ms/step - accuracy: 0.8780 - loss: 0.2894\n",
      "Epoch 67: val_accuracy improved from 0.90670 to 0.91046, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 856ms/step - accuracy: 0.8780 - loss: 0.2898 - val_accuracy: 0.9105 - val_loss: 0.2202 - learning_rate: 0.0020\n",
      "Epoch 68/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 815ms/step - accuracy: 0.8730 - loss: 0.3047\n",
      "Epoch 68: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 900ms/step - accuracy: 0.8733 - loss: 0.3042 - val_accuracy: 0.8912 - val_loss: 0.2418 - learning_rate: 0.0020\n",
      "Epoch 69/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777ms/step - accuracy: 0.8847 - loss: 0.2878\n",
      "Epoch 69: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 867ms/step - accuracy: 0.8845 - loss: 0.2882 - val_accuracy: 0.8976 - val_loss: 0.2306 - learning_rate: 0.0020\n",
      "Epoch 70/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772ms/step - accuracy: 0.8787 - loss: 0.2920\n",
      "Epoch 70: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 852ms/step - accuracy: 0.8788 - loss: 0.2918 - val_accuracy: 0.9029 - val_loss: 0.2223 - learning_rate: 0.0020\n",
      "Epoch 71/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760ms/step - accuracy: 0.8848 - loss: 0.2785\n",
      "Epoch 71: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 846ms/step - accuracy: 0.8847 - loss: 0.2789 - val_accuracy: 0.9056 - val_loss: 0.2135 - learning_rate: 0.0020\n",
      "Epoch 72/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 801ms/step - accuracy: 0.8783 - loss: 0.2934\n",
      "Epoch 72: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 894ms/step - accuracy: 0.8785 - loss: 0.2930 - val_accuracy: 0.9062 - val_loss: 0.2225 - learning_rate: 0.0020\n",
      "Epoch 73/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796ms/step - accuracy: 0.8917 - loss: 0.2701\n",
      "Epoch 73: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 881ms/step - accuracy: 0.8915 - loss: 0.2705 - val_accuracy: 0.9094 - val_loss: 0.2315 - learning_rate: 0.0020\n",
      "Epoch 74/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822ms/step - accuracy: 0.8855 - loss: 0.2915\n",
      "Epoch 74: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 908ms/step - accuracy: 0.8857 - loss: 0.2912 - val_accuracy: 0.8997 - val_loss: 0.2336 - learning_rate: 0.0020\n",
      "Epoch 75/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849ms/step - accuracy: 0.8823 - loss: 0.2839\n",
      "Epoch 75: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 942ms/step - accuracy: 0.8824 - loss: 0.2840 - val_accuracy: 0.9035 - val_loss: 0.2348 - learning_rate: 0.0020\n",
      "Epoch 76/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794ms/step - accuracy: 0.8956 - loss: 0.2679\n",
      "Epoch 76: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 885ms/step - accuracy: 0.8955 - loss: 0.2680 - val_accuracy: 0.9083 - val_loss: 0.2149 - learning_rate: 0.0020\n",
      "Epoch 77/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779ms/step - accuracy: 0.8980 - loss: 0.2601\n",
      "Epoch 77: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 865ms/step - accuracy: 0.8979 - loss: 0.2603 - val_accuracy: 0.9003 - val_loss: 0.2214 - learning_rate: 0.0020\n",
      "Epoch 78/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769ms/step - accuracy: 0.8835 - loss: 0.2791\n",
      "Epoch 78: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 856ms/step - accuracy: 0.8837 - loss: 0.2792 - val_accuracy: 0.9067 - val_loss: 0.2142 - learning_rate: 0.0020\n",
      "Epoch 79/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782ms/step - accuracy: 0.8915 - loss: 0.2699\n",
      "Epoch 79: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 867ms/step - accuracy: 0.8914 - loss: 0.2699 - val_accuracy: 0.9029 - val_loss: 0.2174 - learning_rate: 0.0020\n",
      "Epoch 80/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871ms/step - accuracy: 0.8826 - loss: 0.2906\n",
      "Epoch 80: val_accuracy did not improve from 0.91046\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 964ms/step - accuracy: 0.8827 - loss: 0.2906 - val_accuracy: 0.9067 - val_loss: 0.2179 - learning_rate: 0.0020\n",
      "Epoch 81/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855ms/step - accuracy: 0.8971 - loss: 0.2681\n",
      "Epoch 81: val_accuracy improved from 0.91046 to 0.91689, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 957ms/step - accuracy: 0.8969 - loss: 0.2682 - val_accuracy: 0.9169 - val_loss: 0.2050 - learning_rate: 0.0020\n",
      "Epoch 82/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 882ms/step - accuracy: 0.8916 - loss: 0.2613\n",
      "Epoch 82: val_accuracy did not improve from 0.91689\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 980ms/step - accuracy: 0.8915 - loss: 0.2616 - val_accuracy: 0.9046 - val_loss: 0.2086 - learning_rate: 0.0020\n",
      "Epoch 83/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871ms/step - accuracy: 0.8900 - loss: 0.2777\n",
      "Epoch 83: val_accuracy improved from 0.91689 to 0.91743, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 972ms/step - accuracy: 0.8901 - loss: 0.2775 - val_accuracy: 0.9174 - val_loss: 0.2015 - learning_rate: 0.0020\n",
      "Epoch 84/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865ms/step - accuracy: 0.8959 - loss: 0.2584\n",
      "Epoch 84: val_accuracy improved from 0.91743 to 0.92225, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 961ms/step - accuracy: 0.8959 - loss: 0.2585 - val_accuracy: 0.9223 - val_loss: 0.2005 - learning_rate: 0.0020\n",
      "Epoch 85/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877ms/step - accuracy: 0.8979 - loss: 0.2551\n",
      "Epoch 85: val_accuracy did not improve from 0.92225\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 974ms/step - accuracy: 0.8980 - loss: 0.2553 - val_accuracy: 0.9024 - val_loss: 0.2188 - learning_rate: 0.0020\n",
      "Epoch 86/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894ms/step - accuracy: 0.8952 - loss: 0.2677\n",
      "Epoch 86: val_accuracy did not improve from 0.92225\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8954 - loss: 0.2671 - val_accuracy: 0.9158 - val_loss: 0.2006 - learning_rate: 0.0020\n",
      "Epoch 87/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914ms/step - accuracy: 0.9012 - loss: 0.2521\n",
      "Epoch 87: val_accuracy did not improve from 0.92225\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9013 - loss: 0.2521 - val_accuracy: 0.9217 - val_loss: 0.2063 - learning_rate: 0.0020\n",
      "Epoch 88/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884ms/step - accuracy: 0.8993 - loss: 0.2597\n",
      "Epoch 88: val_accuracy did not improve from 0.92225\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 981ms/step - accuracy: 0.8993 - loss: 0.2598 - val_accuracy: 0.9164 - val_loss: 0.2060 - learning_rate: 0.0020\n",
      "Epoch 89/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874ms/step - accuracy: 0.8987 - loss: 0.2532\n",
      "Epoch 89: val_accuracy improved from 0.92225 to 0.92332, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 975ms/step - accuracy: 0.8986 - loss: 0.2533 - val_accuracy: 0.9233 - val_loss: 0.2065 - learning_rate: 0.0020\n",
      "Epoch 90/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889ms/step - accuracy: 0.9066 - loss: 0.2440\n",
      "Epoch 90: val_accuracy improved from 0.92332 to 0.92547, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 984ms/step - accuracy: 0.9066 - loss: 0.2437 - val_accuracy: 0.9255 - val_loss: 0.1916 - learning_rate: 0.0020\n",
      "Epoch 91/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902ms/step - accuracy: 0.9060 - loss: 0.2372\n",
      "Epoch 91: val_accuracy did not improve from 0.92547\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9058 - loss: 0.2378 - val_accuracy: 0.9228 - val_loss: 0.1934 - learning_rate: 0.0020\n",
      "Epoch 92/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 906ms/step - accuracy: 0.8953 - loss: 0.2644\n",
      "Epoch 92: val_accuracy did not improve from 0.92547\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8955 - loss: 0.2641 - val_accuracy: 0.9212 - val_loss: 0.1908 - learning_rate: 0.0020\n",
      "Epoch 93/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867ms/step - accuracy: 0.9021 - loss: 0.2469\n",
      "Epoch 93: val_accuracy did not improve from 0.92547\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 954ms/step - accuracy: 0.9020 - loss: 0.2473 - val_accuracy: 0.9147 - val_loss: 0.2123 - learning_rate: 0.0020\n",
      "Epoch 94/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 872ms/step - accuracy: 0.9043 - loss: 0.2643\n",
      "Epoch 94: val_accuracy did not improve from 0.92547\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 967ms/step - accuracy: 0.9045 - loss: 0.2636 - val_accuracy: 0.9233 - val_loss: 0.2019 - learning_rate: 0.0020\n",
      "Epoch 95/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850ms/step - accuracy: 0.9121 - loss: 0.2271\n",
      "Epoch 95: val_accuracy improved from 0.92547 to 0.92601, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 952ms/step - accuracy: 0.9121 - loss: 0.2272 - val_accuracy: 0.9260 - val_loss: 0.1894 - learning_rate: 0.0020\n",
      "Epoch 96/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899ms/step - accuracy: 0.9061 - loss: 0.2414\n",
      "Epoch 96: val_accuracy improved from 0.92601 to 0.92654, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 992ms/step - accuracy: 0.9062 - loss: 0.2413 - val_accuracy: 0.9265 - val_loss: 0.1924 - learning_rate: 0.0020\n",
      "Epoch 97/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 877ms/step - accuracy: 0.9084 - loss: 0.2301\n",
      "Epoch 97: val_accuracy improved from 0.92654 to 0.92869, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 975ms/step - accuracy: 0.9086 - loss: 0.2301 - val_accuracy: 0.9287 - val_loss: 0.1821 - learning_rate: 0.0020\n",
      "Epoch 98/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905ms/step - accuracy: 0.9159 - loss: 0.2134\n",
      "Epoch 98: val_accuracy improved from 0.92869 to 0.92976, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 997ms/step - accuracy: 0.9159 - loss: 0.2136 - val_accuracy: 0.9298 - val_loss: 0.1886 - learning_rate: 0.0020\n",
      "Epoch 99/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937ms/step - accuracy: 0.9086 - loss: 0.2342\n",
      "Epoch 99: val_accuracy did not improve from 0.92976\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9087 - loss: 0.2343 - val_accuracy: 0.9169 - val_loss: 0.1986 - learning_rate: 0.0020\n",
      "Epoch 100/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969ms/step - accuracy: 0.9185 - loss: 0.2144\n",
      "Epoch 100: val_accuracy improved from 0.92976 to 0.93405, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9183 - loss: 0.2147 - val_accuracy: 0.9340 - val_loss: 0.1995 - learning_rate: 0.0020\n",
      "Epoch 101/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958ms/step - accuracy: 0.9121 - loss: 0.2293\n",
      "Epoch 101: val_accuracy did not improve from 0.93405\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9121 - loss: 0.2288 - val_accuracy: 0.9158 - val_loss: 0.2057 - learning_rate: 0.0020\n",
      "Epoch 102/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970ms/step - accuracy: 0.9121 - loss: 0.2264\n",
      "Epoch 102: val_accuracy did not improve from 0.93405\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9122 - loss: 0.2262 - val_accuracy: 0.9314 - val_loss: 0.1903 - learning_rate: 0.0020\n",
      "Epoch 103/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936ms/step - accuracy: 0.9075 - loss: 0.2338\n",
      "Epoch 103: val_accuracy improved from 0.93405 to 0.93673, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9076 - loss: 0.2335 - val_accuracy: 0.9367 - val_loss: 0.1623 - learning_rate: 0.0020\n",
      "Epoch 104/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939ms/step - accuracy: 0.9190 - loss: 0.2115\n",
      "Epoch 104: val_accuracy did not improve from 0.93673\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9190 - loss: 0.2118 - val_accuracy: 0.9319 - val_loss: 0.1884 - learning_rate: 0.0020\n",
      "Epoch 105/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918ms/step - accuracy: 0.9156 - loss: 0.2257\n",
      "Epoch 105: val_accuracy improved from 0.93673 to 0.94102, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9156 - loss: 0.2253 - val_accuracy: 0.9410 - val_loss: 0.1665 - learning_rate: 0.0020\n",
      "Epoch 106/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 952ms/step - accuracy: 0.9210 - loss: 0.2036\n",
      "Epoch 106: val_accuracy did not improve from 0.94102\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9211 - loss: 0.2039 - val_accuracy: 0.9303 - val_loss: 0.1781 - learning_rate: 0.0020\n",
      "Epoch 107/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973ms/step - accuracy: 0.9235 - loss: 0.2102\n",
      "Epoch 107: val_accuracy did not improve from 0.94102\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9232 - loss: 0.2107 - val_accuracy: 0.9239 - val_loss: 0.1998 - learning_rate: 0.0020\n",
      "Epoch 108/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938ms/step - accuracy: 0.9130 - loss: 0.2231\n",
      "Epoch 108: val_accuracy did not improve from 0.94102\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9130 - loss: 0.2231 - val_accuracy: 0.9276 - val_loss: 0.1873 - learning_rate: 0.0020\n",
      "Epoch 109/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955ms/step - accuracy: 0.9150 - loss: 0.2235\n",
      "Epoch 109: val_accuracy did not improve from 0.94102\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9149 - loss: 0.2235 - val_accuracy: 0.9383 - val_loss: 0.1616 - learning_rate: 0.0020\n",
      "Epoch 110/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903ms/step - accuracy: 0.9240 - loss: 0.2099\n",
      "Epoch 110: val_accuracy did not improve from 0.94102\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 998ms/step - accuracy: 0.9239 - loss: 0.2096 - val_accuracy: 0.9383 - val_loss: 0.1737 - learning_rate: 0.0020\n",
      "Epoch 111/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903ms/step - accuracy: 0.9250 - loss: 0.1984\n",
      "Epoch 111: val_accuracy did not improve from 0.94102\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9251 - loss: 0.1983 - val_accuracy: 0.9346 - val_loss: 0.1582 - learning_rate: 0.0020\n",
      "Epoch 112/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909ms/step - accuracy: 0.9300 - loss: 0.1973\n",
      "Epoch 112: val_accuracy did not improve from 0.94102\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9297 - loss: 0.1975 - val_accuracy: 0.9405 - val_loss: 0.1455 - learning_rate: 0.0020\n",
      "Epoch 113/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905ms/step - accuracy: 0.9242 - loss: 0.2002\n",
      "Epoch 113: val_accuracy did not improve from 0.94102\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9241 - loss: 0.2005 - val_accuracy: 0.9394 - val_loss: 0.1603 - learning_rate: 0.0020\n",
      "Epoch 114/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927ms/step - accuracy: 0.9248 - loss: 0.1968\n",
      "Epoch 114: val_accuracy did not improve from 0.94102\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9249 - loss: 0.1966 - val_accuracy: 0.9373 - val_loss: 0.1607 - learning_rate: 0.0020\n",
      "Epoch 115/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889ms/step - accuracy: 0.9286 - loss: 0.1943\n",
      "Epoch 115: val_accuracy did not improve from 0.94102\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 990ms/step - accuracy: 0.9283 - loss: 0.1947 - val_accuracy: 0.9389 - val_loss: 0.1682 - learning_rate: 0.0020\n",
      "Epoch 116/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925ms/step - accuracy: 0.9192 - loss: 0.2077\n",
      "Epoch 116: val_accuracy improved from 0.94102 to 0.94263, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9194 - loss: 0.2074 - val_accuracy: 0.9426 - val_loss: 0.1560 - learning_rate: 0.0020\n",
      "Epoch 117/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968ms/step - accuracy: 0.9268 - loss: 0.1971\n",
      "Epoch 117: val_accuracy did not improve from 0.94263\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9269 - loss: 0.1968 - val_accuracy: 0.9389 - val_loss: 0.1623 - learning_rate: 0.0020\n",
      "Epoch 118/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947ms/step - accuracy: 0.9279 - loss: 0.1905\n",
      "Epoch 118: val_accuracy improved from 0.94263 to 0.94799, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9278 - loss: 0.1907 - val_accuracy: 0.9480 - val_loss: 0.1469 - learning_rate: 0.0020\n",
      "Epoch 119/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989ms/step - accuracy: 0.9307 - loss: 0.1925\n",
      "Epoch 119: val_accuracy did not improve from 0.94799\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9306 - loss: 0.1925 - val_accuracy: 0.9426 - val_loss: 0.1514 - learning_rate: 0.0020\n",
      "Epoch 120/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946ms/step - accuracy: 0.9236 - loss: 0.2039\n",
      "Epoch 120: val_accuracy did not improve from 0.94799\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9236 - loss: 0.2039 - val_accuracy: 0.9383 - val_loss: 0.1490 - learning_rate: 0.0020\n",
      "Epoch 121/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937ms/step - accuracy: 0.9202 - loss: 0.2057\n",
      "Epoch 121: val_accuracy did not improve from 0.94799\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9203 - loss: 0.2055 - val_accuracy: 0.9383 - val_loss: 0.1629 - learning_rate: 0.0020\n",
      "Epoch 122/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896ms/step - accuracy: 0.9301 - loss: 0.1926\n",
      "Epoch 122: val_accuracy did not improve from 0.94799\n",
      "\n",
      "Epoch 122: ReduceLROnPlateau reducing learning rate to 0.0019000000902451575.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 994ms/step - accuracy: 0.9301 - loss: 0.1929 - val_accuracy: 0.9453 - val_loss: 0.1532 - learning_rate: 0.0020\n",
      "Epoch 123/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921ms/step - accuracy: 0.9292 - loss: 0.1949\n",
      "Epoch 123: val_accuracy did not improve from 0.94799\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9292 - loss: 0.1948 - val_accuracy: 0.9437 - val_loss: 0.1527 - learning_rate: 0.0019\n",
      "Epoch 124/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895ms/step - accuracy: 0.9272 - loss: 0.1929\n",
      "Epoch 124: val_accuracy did not improve from 0.94799\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 993ms/step - accuracy: 0.9271 - loss: 0.1931 - val_accuracy: 0.9437 - val_loss: 0.1536 - learning_rate: 0.0019\n",
      "Epoch 125/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 904ms/step - accuracy: 0.9251 - loss: 0.1998\n",
      "Epoch 125: val_accuracy did not improve from 0.94799\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9254 - loss: 0.1993 - val_accuracy: 0.9367 - val_loss: 0.1765 - learning_rate: 0.0019\n",
      "Epoch 126/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910ms/step - accuracy: 0.9280 - loss: 0.1907\n",
      "Epoch 126: val_accuracy did not improve from 0.94799\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9281 - loss: 0.1906 - val_accuracy: 0.9405 - val_loss: 0.1571 - learning_rate: 0.0019\n",
      "Epoch 127/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918ms/step - accuracy: 0.9237 - loss: 0.1981\n",
      "Epoch 127: val_accuracy did not improve from 0.94799\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9238 - loss: 0.1983 - val_accuracy: 0.9330 - val_loss: 0.1827 - learning_rate: 0.0019\n",
      "Epoch 128/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918ms/step - accuracy: 0.9305 - loss: 0.1983\n",
      "Epoch 128: val_accuracy did not improve from 0.94799\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9307 - loss: 0.1980 - val_accuracy: 0.9458 - val_loss: 0.1525 - learning_rate: 0.0019\n",
      "Epoch 129/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941ms/step - accuracy: 0.9349 - loss: 0.1746\n",
      "Epoch 129: val_accuracy did not improve from 0.94799\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9349 - loss: 0.1749 - val_accuracy: 0.9480 - val_loss: 0.1473 - learning_rate: 0.0019\n",
      "Epoch 130/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 915ms/step - accuracy: 0.9337 - loss: 0.1790\n",
      "Epoch 130: val_accuracy improved from 0.94799 to 0.94906, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9337 - loss: 0.1793 - val_accuracy: 0.9491 - val_loss: 0.1332 - learning_rate: 0.0019\n",
      "Epoch 131/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937ms/step - accuracy: 0.9393 - loss: 0.1736\n",
      "Epoch 131: val_accuracy improved from 0.94906 to 0.95067, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9391 - loss: 0.1741 - val_accuracy: 0.9507 - val_loss: 0.1340 - learning_rate: 0.0019\n",
      "Epoch 132/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936ms/step - accuracy: 0.9387 - loss: 0.1733\n",
      "Epoch 132: val_accuracy did not improve from 0.95067\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9386 - loss: 0.1736 - val_accuracy: 0.9507 - val_loss: 0.1250 - learning_rate: 0.0019\n",
      "Epoch 133/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924ms/step - accuracy: 0.9334 - loss: 0.1780\n",
      "Epoch 133: val_accuracy did not improve from 0.95067\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9332 - loss: 0.1786 - val_accuracy: 0.9255 - val_loss: 0.1756 - learning_rate: 0.0019\n",
      "Epoch 134/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 913ms/step - accuracy: 0.9269 - loss: 0.1861\n",
      "Epoch 134: val_accuracy did not improve from 0.95067\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9268 - loss: 0.1867 - val_accuracy: 0.9378 - val_loss: 0.1655 - learning_rate: 0.0019\n",
      "Epoch 135/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 938ms/step - accuracy: 0.9325 - loss: 0.1895\n",
      "Epoch 135: val_accuracy did not improve from 0.95067\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9326 - loss: 0.1892 - val_accuracy: 0.9464 - val_loss: 0.1466 - learning_rate: 0.0019\n",
      "Epoch 136/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947ms/step - accuracy: 0.9440 - loss: 0.1585\n",
      "Epoch 136: val_accuracy did not improve from 0.95067\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9440 - loss: 0.1587 - val_accuracy: 0.9453 - val_loss: 0.1462 - learning_rate: 0.0019\n",
      "Epoch 137/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918ms/step - accuracy: 0.9338 - loss: 0.1773\n",
      "Epoch 137: val_accuracy did not improve from 0.95067\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9338 - loss: 0.1775 - val_accuracy: 0.9475 - val_loss: 0.1555 - learning_rate: 0.0019\n",
      "Epoch 138/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922ms/step - accuracy: 0.9440 - loss: 0.1541\n",
      "Epoch 138: val_accuracy did not improve from 0.95067\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9439 - loss: 0.1545 - val_accuracy: 0.9453 - val_loss: 0.1488 - learning_rate: 0.0019\n",
      "Epoch 139/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949ms/step - accuracy: 0.9421 - loss: 0.1673\n",
      "Epoch 139: val_accuracy did not improve from 0.95067\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9420 - loss: 0.1674 - val_accuracy: 0.9330 - val_loss: 0.1782 - learning_rate: 0.0019\n",
      "Epoch 140/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941ms/step - accuracy: 0.9388 - loss: 0.1777\n",
      "Epoch 140: val_accuracy did not improve from 0.95067\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9388 - loss: 0.1777 - val_accuracy: 0.9373 - val_loss: 0.1591 - learning_rate: 0.0019\n",
      "Epoch 141/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916ms/step - accuracy: 0.9371 - loss: 0.1651\n",
      "Epoch 141: val_accuracy did not improve from 0.95067\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9371 - loss: 0.1651 - val_accuracy: 0.9501 - val_loss: 0.1318 - learning_rate: 0.0019\n",
      "Epoch 142/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916ms/step - accuracy: 0.9390 - loss: 0.1606\n",
      "Epoch 142: val_accuracy improved from 0.95067 to 0.95389, saving model to best_weights.keras\n",
      "\n",
      "Epoch 142: ReduceLROnPlateau reducing learning rate to 0.0018050000304356217.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9391 - loss: 0.1607 - val_accuracy: 0.9539 - val_loss: 0.1263 - learning_rate: 0.0019\n",
      "Epoch 143/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941ms/step - accuracy: 0.9435 - loss: 0.1619\n",
      "Epoch 143: val_accuracy did not improve from 0.95389\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9436 - loss: 0.1616 - val_accuracy: 0.9534 - val_loss: 0.1343 - learning_rate: 0.0018\n",
      "Epoch 144/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912ms/step - accuracy: 0.9399 - loss: 0.1588\n",
      "Epoch 144: val_accuracy improved from 0.95389 to 0.95550, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9399 - loss: 0.1591 - val_accuracy: 0.9555 - val_loss: 0.1264 - learning_rate: 0.0018\n",
      "Epoch 145/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936ms/step - accuracy: 0.9428 - loss: 0.1521\n",
      "Epoch 145: val_accuracy improved from 0.95550 to 0.95603, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9429 - loss: 0.1522 - val_accuracy: 0.9560 - val_loss: 0.1276 - learning_rate: 0.0018\n",
      "Epoch 146/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926ms/step - accuracy: 0.9479 - loss: 0.1459\n",
      "Epoch 146: val_accuracy improved from 0.95603 to 0.95657, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9477 - loss: 0.1464 - val_accuracy: 0.9566 - val_loss: 0.1366 - learning_rate: 0.0018\n",
      "Epoch 147/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946ms/step - accuracy: 0.9511 - loss: 0.1400\n",
      "Epoch 147: val_accuracy did not improve from 0.95657\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9509 - loss: 0.1405 - val_accuracy: 0.9517 - val_loss: 0.1335 - learning_rate: 0.0018\n",
      "Epoch 148/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 907ms/step - accuracy: 0.9508 - loss: 0.1393\n",
      "Epoch 148: val_accuracy did not improve from 0.95657\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1000ms/step - accuracy: 0.9507 - loss: 0.1398 - val_accuracy: 0.9534 - val_loss: 0.1289 - learning_rate: 0.0018\n",
      "Epoch 149/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937ms/step - accuracy: 0.9470 - loss: 0.1583\n",
      "Epoch 149: val_accuracy did not improve from 0.95657\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9469 - loss: 0.1585 - val_accuracy: 0.9442 - val_loss: 0.1433 - learning_rate: 0.0018\n",
      "Epoch 150/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930ms/step - accuracy: 0.9397 - loss: 0.1654\n",
      "Epoch 150: val_accuracy did not improve from 0.95657\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9397 - loss: 0.1656 - val_accuracy: 0.9469 - val_loss: 0.1435 - learning_rate: 0.0018\n",
      "Epoch 151/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936ms/step - accuracy: 0.9463 - loss: 0.1561\n",
      "Epoch 151: val_accuracy did not improve from 0.95657\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9462 - loss: 0.1560 - val_accuracy: 0.9544 - val_loss: 0.1333 - learning_rate: 0.0018\n",
      "Epoch 152/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918ms/step - accuracy: 0.9461 - loss: 0.1524\n",
      "Epoch 152: val_accuracy did not improve from 0.95657\n",
      "\n",
      "Epoch 152: ReduceLROnPlateau reducing learning rate to 0.0017147500067949293.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9462 - loss: 0.1521 - val_accuracy: 0.9560 - val_loss: 0.1324 - learning_rate: 0.0018\n",
      "Epoch 153/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909ms/step - accuracy: 0.9522 - loss: 0.1430\n",
      "Epoch 153: val_accuracy did not improve from 0.95657\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9521 - loss: 0.1429 - val_accuracy: 0.9566 - val_loss: 0.1346 - learning_rate: 0.0017\n",
      "Epoch 154/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 909ms/step - accuracy: 0.9561 - loss: 0.1251\n",
      "Epoch 154: val_accuracy improved from 0.95657 to 0.95710, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9558 - loss: 0.1258 - val_accuracy: 0.9571 - val_loss: 0.1241 - learning_rate: 0.0017\n",
      "Epoch 155/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966ms/step - accuracy: 0.9528 - loss: 0.1443\n",
      "Epoch 155: val_accuracy improved from 0.95710 to 0.96247, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9529 - loss: 0.1441 - val_accuracy: 0.9625 - val_loss: 0.1165 - learning_rate: 0.0017\n",
      "Epoch 156/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 945ms/step - accuracy: 0.9493 - loss: 0.1482\n",
      "Epoch 156: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9493 - loss: 0.1483 - val_accuracy: 0.9410 - val_loss: 0.1619 - learning_rate: 0.0017\n",
      "Epoch 157/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921ms/step - accuracy: 0.9460 - loss: 0.1506\n",
      "Epoch 157: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9461 - loss: 0.1504 - val_accuracy: 0.9539 - val_loss: 0.1356 - learning_rate: 0.0017\n",
      "Epoch 158/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958ms/step - accuracy: 0.9495 - loss: 0.1373\n",
      "Epoch 158: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9494 - loss: 0.1376 - val_accuracy: 0.9491 - val_loss: 0.1524 - learning_rate: 0.0017\n",
      "Epoch 159/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971ms/step - accuracy: 0.9547 - loss: 0.1296\n",
      "Epoch 159: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9546 - loss: 0.1298 - val_accuracy: 0.9582 - val_loss: 0.1328 - learning_rate: 0.0017\n",
      "Epoch 160/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973ms/step - accuracy: 0.9571 - loss: 0.1248\n",
      "Epoch 160: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9571 - loss: 0.1248 - val_accuracy: 0.9592 - val_loss: 0.1217 - learning_rate: 0.0017\n",
      "Epoch 161/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918ms/step - accuracy: 0.9559 - loss: 0.1291\n",
      "Epoch 161: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9559 - loss: 0.1290 - val_accuracy: 0.9609 - val_loss: 0.1198 - learning_rate: 0.0017\n",
      "Epoch 162/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969ms/step - accuracy: 0.9608 - loss: 0.1153\n",
      "Epoch 162: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9607 - loss: 0.1159 - val_accuracy: 0.9603 - val_loss: 0.1284 - learning_rate: 0.0017\n",
      "Epoch 163/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916ms/step - accuracy: 0.9525 - loss: 0.1424\n",
      "Epoch 163: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9523 - loss: 0.1427 - val_accuracy: 0.9550 - val_loss: 0.1397 - learning_rate: 0.0017\n",
      "Epoch 164/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965ms/step - accuracy: 0.9531 - loss: 0.1331\n",
      "Epoch 164: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9530 - loss: 0.1331 - val_accuracy: 0.9603 - val_loss: 0.1284 - learning_rate: 0.0017\n",
      "Epoch 165/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963ms/step - accuracy: 0.9478 - loss: 0.1444\n",
      "Epoch 165: val_accuracy did not improve from 0.96247\n",
      "\n",
      "Epoch 165: ReduceLROnPlateau reducing learning rate to 0.0016290124622173607.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9480 - loss: 0.1441 - val_accuracy: 0.9560 - val_loss: 0.1361 - learning_rate: 0.0017\n",
      "Epoch 166/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994ms/step - accuracy: 0.9567 - loss: 0.1231\n",
      "Epoch 166: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9566 - loss: 0.1234 - val_accuracy: 0.9544 - val_loss: 0.1240 - learning_rate: 0.0016\n",
      "Epoch 167/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896ms/step - accuracy: 0.9550 - loss: 0.1344\n",
      "Epoch 167: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 990ms/step - accuracy: 0.9551 - loss: 0.1342 - val_accuracy: 0.9346 - val_loss: 0.1832 - learning_rate: 0.0016\n",
      "Epoch 168/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917ms/step - accuracy: 0.9505 - loss: 0.1457\n",
      "Epoch 168: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9506 - loss: 0.1455 - val_accuracy: 0.9469 - val_loss: 0.1483 - learning_rate: 0.0016\n",
      "Epoch 169/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897ms/step - accuracy: 0.9455 - loss: 0.1445\n",
      "Epoch 169: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 994ms/step - accuracy: 0.9458 - loss: 0.1439 - val_accuracy: 0.9582 - val_loss: 0.1256 - learning_rate: 0.0016\n",
      "Epoch 170/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937ms/step - accuracy: 0.9568 - loss: 0.1291\n",
      "Epoch 170: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9569 - loss: 0.1289 - val_accuracy: 0.9475 - val_loss: 0.1614 - learning_rate: 0.0016\n",
      "Epoch 171/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953ms/step - accuracy: 0.9551 - loss: 0.1335\n",
      "Epoch 171: val_accuracy did not improve from 0.96247\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9551 - loss: 0.1334 - val_accuracy: 0.9560 - val_loss: 0.1487 - learning_rate: 0.0016\n",
      "Epoch 172/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964ms/step - accuracy: 0.9605 - loss: 0.1232\n",
      "Epoch 172: val_accuracy improved from 0.96247 to 0.96300, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9603 - loss: 0.1235 - val_accuracy: 0.9630 - val_loss: 0.1153 - learning_rate: 0.0016\n",
      "Epoch 173/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990ms/step - accuracy: 0.9559 - loss: 0.1242\n",
      "Epoch 173: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9558 - loss: 0.1244 - val_accuracy: 0.9560 - val_loss: 0.1281 - learning_rate: 0.0016\n",
      "Epoch 174/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967ms/step - accuracy: 0.9450 - loss: 0.1513\n",
      "Epoch 174: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9452 - loss: 0.1509 - val_accuracy: 0.9491 - val_loss: 0.1340 - learning_rate: 0.0016\n",
      "Epoch 175/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949ms/step - accuracy: 0.9566 - loss: 0.1254\n",
      "Epoch 175: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9565 - loss: 0.1255 - val_accuracy: 0.9598 - val_loss: 0.1223 - learning_rate: 0.0016\n",
      "Epoch 176/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946ms/step - accuracy: 0.9604 - loss: 0.1158\n",
      "Epoch 176: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9604 - loss: 0.1158 - val_accuracy: 0.9592 - val_loss: 0.1271 - learning_rate: 0.0016\n",
      "Epoch 177/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9594 - loss: 0.1277\n",
      "Epoch 177: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9594 - loss: 0.1275 - val_accuracy: 0.9582 - val_loss: 0.1311 - learning_rate: 0.0016\n",
      "Epoch 178/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893ms/step - accuracy: 0.9579 - loss: 0.1149\n",
      "Epoch 178: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 977ms/step - accuracy: 0.9578 - loss: 0.1155 - val_accuracy: 0.9480 - val_loss: 0.1452 - learning_rate: 0.0016\n",
      "Epoch 179/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859ms/step - accuracy: 0.9593 - loss: 0.1213\n",
      "Epoch 179: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 938ms/step - accuracy: 0.9594 - loss: 0.1210 - val_accuracy: 0.9603 - val_loss: 0.1276 - learning_rate: 0.0016\n",
      "Epoch 180/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850ms/step - accuracy: 0.9599 - loss: 0.1226\n",
      "Epoch 180: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 949ms/step - accuracy: 0.9599 - loss: 0.1223 - val_accuracy: 0.9614 - val_loss: 0.1239 - learning_rate: 0.0016\n",
      "Epoch 181/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9602 - loss: 0.1130\n",
      "Epoch 181: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9602 - loss: 0.1131 - val_accuracy: 0.9539 - val_loss: 0.1425 - learning_rate: 0.0016\n",
      "Epoch 182/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9545 - loss: 0.1235\n",
      "Epoch 182: val_accuracy did not improve from 0.96300\n",
      "\n",
      "Epoch 182: ReduceLROnPlateau reducing learning rate to 0.0015475617838092148.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9545 - loss: 0.1234 - val_accuracy: 0.9560 - val_loss: 0.1363 - learning_rate: 0.0016\n",
      "Epoch 183/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886ms/step - accuracy: 0.9646 - loss: 0.1073\n",
      "Epoch 183: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 969ms/step - accuracy: 0.9645 - loss: 0.1076 - val_accuracy: 0.9625 - val_loss: 0.1125 - learning_rate: 0.0015\n",
      "Epoch 184/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 828ms/step - accuracy: 0.9632 - loss: 0.1154\n",
      "Epoch 184: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 923ms/step - accuracy: 0.9631 - loss: 0.1154 - val_accuracy: 0.9491 - val_loss: 0.1533 - learning_rate: 0.0015\n",
      "Epoch 185/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975ms/step - accuracy: 0.9638 - loss: 0.1083\n",
      "Epoch 185: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9636 - loss: 0.1085 - val_accuracy: 0.9566 - val_loss: 0.1300 - learning_rate: 0.0015\n",
      "Epoch 186/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9617 - loss: 0.1171 \n",
      "Epoch 186: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9616 - loss: 0.1172 - val_accuracy: 0.9587 - val_loss: 0.1273 - learning_rate: 0.0015\n",
      "Epoch 187/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996ms/step - accuracy: 0.9615 - loss: 0.1141\n",
      "Epoch 187: val_accuracy did not improve from 0.96300\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9615 - loss: 0.1139 - val_accuracy: 0.9550 - val_loss: 0.1258 - learning_rate: 0.0015\n",
      "Epoch 188/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875ms/step - accuracy: 0.9606 - loss: 0.1170\n",
      "Epoch 188: val_accuracy improved from 0.96300 to 0.96568, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 972ms/step - accuracy: 0.9605 - loss: 0.1176 - val_accuracy: 0.9657 - val_loss: 0.1093 - learning_rate: 0.0015\n",
      "Epoch 189/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859ms/step - accuracy: 0.9574 - loss: 0.1212\n",
      "Epoch 189: val_accuracy improved from 0.96568 to 0.96729, saving model to best_weights.keras\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 951ms/step - accuracy: 0.9575 - loss: 0.1207 - val_accuracy: 0.9673 - val_loss: 0.1008 - learning_rate: 0.0015\n",
      "Epoch 190/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892ms/step - accuracy: 0.9642 - loss: 0.1035\n",
      "Epoch 190: val_accuracy did not improve from 0.96729\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 984ms/step - accuracy: 0.9642 - loss: 0.1037 - val_accuracy: 0.9635 - val_loss: 0.1194 - learning_rate: 0.0015\n",
      "Epoch 191/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875ms/step - accuracy: 0.9667 - loss: 0.0959\n",
      "Epoch 191: val_accuracy did not improve from 0.96729\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 969ms/step - accuracy: 0.9667 - loss: 0.0959 - val_accuracy: 0.9603 - val_loss: 0.1285 - learning_rate: 0.0015\n",
      "Epoch 192/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 947ms/step - accuracy: 0.9680 - loss: 0.1010\n",
      "Epoch 192: val_accuracy did not improve from 0.96729\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9678 - loss: 0.1017 - val_accuracy: 0.9630 - val_loss: 0.1299 - learning_rate: 0.0015\n",
      "Epoch 193/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9613 - loss: 0.1126\n",
      "Epoch 193: val_accuracy did not improve from 0.96729\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9613 - loss: 0.1128 - val_accuracy: 0.9598 - val_loss: 0.1169 - learning_rate: 0.0015\n",
      "Epoch 194/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9614 - loss: 0.1016\n",
      "Epoch 194: val_accuracy did not improve from 0.96729\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.9615 - loss: 0.1016 - val_accuracy: 0.9635 - val_loss: 0.1242 - learning_rate: 0.0015\n",
      "Epoch 195/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9657 - loss: 0.1019\n",
      "Epoch 195: val_accuracy did not improve from 0.96729\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9657 - loss: 0.1022 - val_accuracy: 0.9576 - val_loss: 0.1231 - learning_rate: 0.0015\n",
      "Epoch 196/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9643 - loss: 0.1084\n",
      "Epoch 196: val_accuracy did not improve from 0.96729\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9641 - loss: 0.1085 - val_accuracy: 0.9641 - val_loss: 0.1148 - learning_rate: 0.0015\n",
      "Epoch 197/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9623 - loss: 0.1018\n",
      "Epoch 197: val_accuracy did not improve from 0.96729\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9623 - loss: 0.1019 - val_accuracy: 0.9555 - val_loss: 0.1303 - learning_rate: 0.0015\n",
      "Epoch 198/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9651 - loss: 0.1038\n",
      "Epoch 198: val_accuracy did not improve from 0.96729\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9651 - loss: 0.1036 - val_accuracy: 0.9630 - val_loss: 0.1276 - learning_rate: 0.0015\n",
      "Epoch 199/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9711 - loss: 0.0962\n",
      "Epoch 199: val_accuracy did not improve from 0.96729\n",
      "\n",
      "Epoch 199: ReduceLROnPlateau reducing learning rate to 0.001470183639321476.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.9709 - loss: 0.0962 - val_accuracy: 0.9598 - val_loss: 0.1231 - learning_rate: 0.0015\n",
      "Epoch 200/200\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973ms/step - accuracy: 0.9660 - loss: 0.1003\n",
      "Epoch 200: val_accuracy did not improve from 0.96729\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9660 - loss: 0.1005 - val_accuracy: 0.9517 - val_loss: 0.1550 - learning_rate: 0.0015\n",
      "Restoring model weights from the end of the best epoch: 189.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_record = model.fit(x_train, y_train,\n",
    "                            epochs=200,\n",
    "                            batch_size=400,\n",
    "                            callbacks=[early_stopping, model_checkpoint, reduce_lr],  # ReduceLROnPlateau 추가\n",
    "                            validation_split=0.2,\n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 3671.677516222\n"
     ]
    }
   ],
   "source": [
    "end = time()\n",
    "print('time elapsed:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tensorflow adamw CNN-BILSTM_model0430.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(training_record.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(training_record.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "fig.suptitle(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(300, 768, 1), dtype=float64, numpy=\n",
       "array([[[ 0.05100473],\n",
       "        [-0.04711754],\n",
       "        [-0.00292073],\n",
       "        ...,\n",
       "        [ 0.01385851],\n",
       "        [ 0.00762756],\n",
       "        [-0.03124266]],\n",
       "\n",
       "       [[ 0.07115977],\n",
       "        [-0.06056543],\n",
       "        [-0.03135769],\n",
       "        ...,\n",
       "        [ 0.01234996],\n",
       "        [ 0.01655957],\n",
       "        [-0.0149648 ]],\n",
       "\n",
       "       [[ 0.03672672],\n",
       "        [-0.09226788],\n",
       "        [-0.02902521],\n",
       "        ...,\n",
       "        [ 0.01522702],\n",
       "        [ 0.00808274],\n",
       "        [-0.01010432]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.06441222],\n",
       "        [-0.03879228],\n",
       "        [-0.03759597],\n",
       "        ...,\n",
       "        [ 0.03381037],\n",
       "        [ 0.01431669],\n",
       "        [-0.0306724 ]],\n",
       "\n",
       "       [[ 0.02020195],\n",
       "        [-0.03771674],\n",
       "        [-0.01558596],\n",
       "        ...,\n",
       "        [ 0.01634503],\n",
       "        [ 0.01484166],\n",
       "        [-0.02180382]],\n",
       "\n",
       "       [[ 0.05304084],\n",
       "        [-0.03151591],\n",
       "        [-0.02508557],\n",
       "        ...,\n",
       "        [ 0.0263649 ],\n",
       "        [ 0.01496537],\n",
       "        [-0.01457337]]])>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 2, 0, 0,\n",
       "       0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0,\n",
       "       0, 2, 0, 0, 0, 1, 1, 2, 0, 0, 0, 2, 0, 0, 2, 0, 1, 2, 0, 2, 0, 0,\n",
       "       2, 1, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 1, 2,\n",
       "       0, 0, 2, 0, 2, 2, 0, 0, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 0, 1,\n",
       "       2, 0, 2, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 2, 0, 2, 1, 0, 2, 0, 0, 1,\n",
       "       2, 0, 1, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2,\n",
       "       1, 0, 0, 2, 0, 2, 0, 1, 0, 2, 2, 0, 0, 1, 2, 0, 0, 2, 1, 0, 0, 2,\n",
       "       0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 2, 0, 0, 2, 2, 1, 2, 2, 0, 0, 2, 0,\n",
       "       0, 0, 0, 0, 0, 2, 2, 1, 0, 2, 1, 2, 2, 0, 0, 0, 2, 0, 2, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 1, 0, 0,\n",
       "       0, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 1, 0, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(test_x)\n",
    "predicted_classes = np.argmax(pred, axis=1)\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=target.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 0,\n",
       "       0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 0,\n",
       "       0, 2, 0, 0, 0, 1, 1, 2, 0, 0, 0, 2, 0, 0, 2, 0, 1, 2, 0, 2, 0, 0,\n",
       "       0, 1, 0, 1, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 2, 0, 1, 2,\n",
       "       0, 0, 2, 0, 2, 2, 0, 0, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 2, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 1,\n",
       "       0, 0, 2, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 2, 0, 2, 1, 0, 2, 0, 0, 1,\n",
       "       2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 2, 2,\n",
       "       1, 0, 0, 2, 0, 0, 0, 1, 0, 2, 2, 0, 0, 1, 2, 0, 0, 2, 1, 0, 0, 2,\n",
       "       0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 1, 2, 2, 0, 0, 2, 0,\n",
       "       0, 2, 0, 0, 0, 2, 2, 2, 0, 2, 1, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 2, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 1, 0, 0,\n",
       "       0, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 1, 0, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "n_classes = 3  \n",
    "\n",
    "\n",
    "y_test_binarized = label_binarize(test_y, classes=[0, 1, 2])\n",
    "y_score= model.predict(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = dict()\n",
    "recall = dict()\n",
    "prc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], y_score[:, i])\n",
    "    prc_auc[i] = auc(recall[i], precision[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic for multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "             label='PRC curve of class {0} (area = {1:0.2f})'.format(i, prc_auc[i]))\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve for multi-class')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 2, 0, 0,\n",
       "       0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0,\n",
       "       0, 2, 0, 0, 0, 1, 1, 2, 0, 0, 0, 2, 0, 0, 2, 0, 1, 2, 0, 2, 0, 0,\n",
       "       2, 1, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 1, 2,\n",
       "       0, 0, 2, 0, 2, 2, 0, 0, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 0, 1,\n",
       "       2, 0, 2, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 2, 0, 2, 1, 0, 2, 0, 0, 1,\n",
       "       2, 0, 1, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 2,\n",
       "       1, 0, 0, 2, 0, 2, 0, 1, 0, 2, 2, 0, 0, 1, 2, 0, 0, 2, 1, 0, 0, 2,\n",
       "       0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 2, 0, 0, 2, 2, 1, 2, 2, 0, 0, 2, 0,\n",
       "       0, 0, 0, 0, 0, 2, 2, 1, 0, 2, 1, 2, 2, 0, 0, 0, 2, 0, 2, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 1, 0, 0,\n",
       "       0, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 1, 0, 2])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 0,\n",
       "       0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 0,\n",
       "       0, 2, 0, 0, 0, 1, 1, 2, 0, 0, 0, 2, 0, 0, 2, 0, 1, 2, 0, 2, 0, 0,\n",
       "       0, 1, 0, 1, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 2, 0, 1, 2,\n",
       "       0, 0, 2, 0, 2, 2, 0, 0, 0, 1, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 2, 2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 1,\n",
       "       0, 0, 2, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 2, 0, 2, 1, 0, 2, 0, 0, 1,\n",
       "       2, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 2, 2,\n",
       "       1, 0, 0, 2, 0, 0, 0, 1, 0, 2, 2, 0, 0, 1, 2, 0, 0, 2, 1, 0, 0, 2,\n",
       "       0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 1, 2, 2, 0, 0, 2, 0,\n",
       "       0, 2, 0, 0, 0, 2, 2, 2, 0, 2, 1, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 2, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 1, 0, 0,\n",
       "       0, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 1, 0, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y=np.array(test_y)\n",
    "test_y=test_y.reshape(-1)\n",
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(predicted_classes.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-LSTM 3 class Classifier의 정확도는: 0.9433\n"
     ]
    }
   ],
   "source": [
    "print('CNN-LSTM 3 class Classifier의 정확도는: {0:.4f}'.format(accuracy_score(test_y, predicted_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "혼동 행렬:\n",
      "[[174   0   8]\n",
      " [  0  35   0]\n",
      " [  5   4  74]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('혼동 행렬:')\n",
    "print(confusion_matrix(test_y, predicted_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CNN LSTM confusion matrix 0430.txt\", \"w\") as text_file:\n",
    "    print(confusion_matrix(test_y, predicted_classes), file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분류 보고서:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96       182\n",
      "           1       0.90      1.00      0.95        35\n",
      "           2       0.90      0.89      0.90        83\n",
      "\n",
      "    accuracy                           0.94       300\n",
      "   macro avg       0.92      0.95      0.94       300\n",
      "weighted avg       0.94      0.94      0.94       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('분류 보고서:')\n",
    "report=classification_report(test_y, predicted_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CNN-LSTM output 0430.txt\", \"w\") as text_file:\n",
    "    print(classification_report(test_y, predicted_classes,digits=4), file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
